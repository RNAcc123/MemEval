"""
è®°å¿†è¯Šæ–­ç³»ç»Ÿ - å¤šæ¨¡å‹è®¨è®ºç‰ˆ

è¯¥æ¨¡å—å®ç°äº†ä¸€ä¸ªå¤šæ¨¡å‹åˆ†é˜¶æ®µè®¨è®ºæ¡†æ¶ï¼š
- é˜¶æ®µ0: ä¸€è‡´æ€§æ£€æŸ¥ - ä¸‰æ¨¡å‹è®¨è®º
- é˜¶æ®µ1: è®°å¿†æå–è¯Šæ–­ - ä¸‰æ¨¡å‹è®¨è®º
- é˜¶æ®µ2: è®°å¿†æ›´æ–°è¯Šæ–­ - ä¸‰æ¨¡å‹è®¨è®º
- é˜¶æ®µ3: è®°å¿†æ£€ç´¢è¯Šæ–­ - ä¸‰æ¨¡å‹è®¨è®º
- é˜¶æ®µ4: æ¨ç†è¯Šæ–­ - ä¸‰æ¨¡å‹è®¨è®º

æ¯ä¸ªé˜¶æ®µå†…ï¼Œä¸‰ä¸ªæ¨¡å‹å…ˆç‹¬ç«‹åˆ¤æ–­ï¼Œç„¶åè¿›è¡Œå¤šè½®è®¨è®ºè¾¾æˆå…±è¯†æˆ–æŠ•ç¥¨å†³å®š
"""

# æ ‡å‡†åº“å¯¼å…¥
import json
import logging
import os
import re
import time
import warnings
from collections import Counter
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
import datetime
import argparse

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from dotenv import load_dotenv
from requests.exceptions import RequestException, Timeout

# ============================================================================
# é…ç½®å’Œåˆå§‹åŒ–
# ============================================================================

logging.getLogger('grpc').setLevel(logging.ERROR)
warnings.filterwarnings('ignore', module='grpc')

load_dotenv()
os.environ['GRPC_ALTS_CREDENTIALS_ENVIRONMENT_OVERRIDE'] = '1'

# å¯¼å…¥åŸæœ‰æ¨¡å—çš„ç±»å’Œå‡½æ•°
from run_diagnosis import (
    APIConfig, DiagnosisConfig, QAData, MemoryData, DiagnosisStage,
    StageResult, DiagnosisResult, API_CONFIG,
    load_json_file, clean_prompt, extract_json_from_response,
    call_llm_api
)


# ============================================================================
# è®¨è®ºç›¸å…³çš„æ•°æ®ç±»
# ============================================================================

@dataclass
class StageOpinion:
    """å•ä¸ªæ¨¡å‹åœ¨æŸé˜¶æ®µçš„æ„è§"""
    model_name: str
    stage_passed: bool
    label: Optional[str]
    reason: str
    round_num: int
    changed_from_passed: Optional[bool] = None
    changed_from_label: Optional[str] = None


@dataclass
class StageDiscussionResult:
    """æŸé˜¶æ®µè®¨è®ºçš„æœ€ç»ˆç»“æœ"""
    stage: DiagnosisStage
    consensus_reached: bool
    final_passed: bool
    final_label: Optional[str]
    final_reason: str
    total_rounds: int
    discussion_history: List[Dict]


@dataclass
class FullDiscussionResult:
    """å®Œæ•´è¯Šæ–­è®¨è®ºç»“æœ"""
    final_label: Optional[str]
    final_reason: str
    final_stage: DiagnosisStage
    stage_results: Dict[str, StageDiscussionResult]


# ============================================================================
# é˜¶æ®µè®¨è®ºPromptç”Ÿæˆå‡½æ•°
# ============================================================================

def generate_stage0_prompt(qa_data: QAData, other_opinions: List[StageOpinion] = None) -> str:
    """ç”Ÿæˆé˜¶æ®µ0çš„promptï¼ˆä¸€è‡´æ€§æ£€æŸ¥ï¼‰"""
    qa_question_str = json.dumps(qa_data.question, ensure_ascii=False)
    qa_answer_str = json.dumps(qa_data.answer, ensure_ascii=False)
    qa_response_str = json.dumps(qa_data.response, ensure_ascii=False)
    
    base_prompt = f"""
You are an evaluation assistant. Determine whether qa_response is semantically consistent with qa_answer.

Consistency rules:
- All key information in qa_answer must appear in qa_response.
- Missing, incorrect or unclear details make it inconsistent.

Example:
qa_answer: "first weekend of August 2023"
qa_response: "5 August 2023."
â†’ inconsistent (incorrectly narrows the time range)

Now evaluate:
input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
"""
    
    if other_opinions:
        opinions_text = "\n=== Other Models' Opinions ===\n"
        for op in other_opinions:
            opinions_text += f"""
Model ({op.model_name}):
- is_consistent: {op.stage_passed}
- reason: {op.reason}
"""
        base_prompt += opinions_text
        base_prompt += """
Consider the other models' opinions. You may KEEP or CHANGE your judgment.
"""
    
    base_prompt += """
Output:
{
  "is_consistent": true/false,
  "reason": "brief explanation"
}
"""
    return base_prompt


def generate_stage1_prompt(
    qa_data: QAData, 
    memory_data: MemoryData, 
    other_opinions: List[StageOpinion] = None
) -> str:
    """ç”Ÿæˆé˜¶æ®µ1çš„promptï¼ˆè®°å¿†æå–ï¼‰"""
    qa_question_str = json.dumps(qa_data.question, ensure_ascii=False)
    qa_answer_str = json.dumps(qa_data.answer, ensure_ascii=False)
    qa_response_str = json.dumps(qa_data.response, ensure_ascii=False)
    
    memories1_initial_results = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "initial_results": item.get("initial_results", []),
        }
        for item in memory_data.person1_memories
    ]
    memories2_initial_results = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "initial_results": item.get("initial_results", []),
        }
        for item in memory_data.person2_memories
    ]
    memories1_str = json.dumps(memories1_initial_results, ensure_ascii=False)
    memories2_str = json.dumps(memories2_initial_results, ensure_ascii=False)
    
    base_prompt = f"""
You are an evaluation assistant for the Memory Extraction Stage.
Task:
1. Use their initial_results (and time_stamp if needed) to determine whether the extracted memories are sufficient to answer qa_question.
2. If sufficient â†’ is_sufficient = true (label = null).
3. If insufficient, classify the issue:
   - "1.1": Missing key information
   - "1.2": Incorrect or conflicting information
   - "1.3": Ambiguous or overly generic information

Examples:

Example 1:
qa_question: "Where did Caroline move from 4 years ago?"
qa_answer: "Sweden"
qa_response: "home country"
person1_memories: {{"initial_results": ["Caroline moved from her home country 4 years ago"]}}
person2_memories: {{"initial_results": []}}
Output:
{{
  "is_sufficient": false,
  "label": "1.1",
  "reason": "The extracted memory only says 'home country' and lacks the specific detail 'Sweden.'"
}}

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- person1_memories: {memories1_str}
- person2_memories: {memories2_str}
"""
    
    if other_opinions:
        opinions_text = "\n=== Other Models' Opinions ===\n"
        for op in other_opinions:
            opinions_text += f"""
Model ({op.model_name}):
- is_sufficient: {op.stage_passed}
- label: {op.label}
- reason: {op.reason}
"""
        base_prompt += opinions_text
        base_prompt += """
Consider the other models' opinions carefully. You may KEEP or CHANGE your judgment.
"""
    
    base_prompt += """
Output format:
{
  "is_sufficient": true/false,
  "label": "1.1" or "1.2" or "1.3" or null,
  "reason": "Detailed explanation"
}
"""
    return base_prompt


def generate_stage2_prompt(
    qa_data: QAData, 
    memory_data: MemoryData, 
    other_opinions: List[StageOpinion] = None
) -> str:
    """ç”Ÿæˆé˜¶æ®µ2çš„promptï¼ˆè®°å¿†æ›´æ–°ï¼‰"""
    qa_question_str = json.dumps(qa_data.question, ensure_ascii=False)
    qa_answer_str = json.dumps(qa_data.answer, ensure_ascii=False)
    qa_response_str = json.dumps(qa_data.response, ensure_ascii=False)
    
    memories1_update_chains = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "update_chain": item.get("update_chain", []),
        }
        for item in memory_data.person1_memories
    ]
    memories2_update_chains = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "update_chain": item.get("update_chain", []),
        }
        for item in memory_data.person2_memories
    ]
    memories1_str = json.dumps(memories1_update_chains, ensure_ascii=False)
    memories2_str = json.dumps(memories2_update_chains, ensure_ascii=False)
    
    base_prompt = f"""
You are an evaluation assistant for the Memory Update Stage.
Task:
1. From the update_chain, use only the final updated memory for each item.
2. Determine whether the updated memories contain sufficient and correct information to answer qa_question.
3. If sufficient â†’ is_sufficient = true (label = null).
4. If insufficient, classify the issue according to the update error type:
   - "2.1": Incorrect update (added wrong or fabricated details)
   - "2.2": Deleted information (removed necessary memory entries)
   - "2.3": Weakened information (kept but diluted or less specific)

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- person1_memories: {memories1_str}
- person2_memories: {memories2_str}
"""
    
    if other_opinions:
        opinions_text = "\n=== Other Models' Opinions ===\n"
        for op in other_opinions:
            opinions_text += f"""
Model ({op.model_name}):
- is_sufficient: {op.stage_passed}
- label: {op.label}
- reason: {op.reason}
"""
        base_prompt += opinions_text
        base_prompt += """
Consider the other models' opinions carefully. You may KEEP or CHANGE your judgment.
"""
    
    base_prompt += """
Output format:
{
  "is_sufficient": true/false,
  "label": "2.1" or "2.2" or "2.3" or null,
  "reason": "Detailed explanation"
}
"""
    return base_prompt


def generate_stage3_prompt(
    qa_data: QAData, 
    memory_data: MemoryData, 
    other_opinions: List[StageOpinion] = None
) -> str:
    """ç”Ÿæˆé˜¶æ®µ3çš„promptï¼ˆè®°å¿†æ£€ç´¢ï¼‰"""
    qa_question_str = json.dumps(qa_data.question, ensure_ascii=False)
    qa_answer_str = json.dumps(qa_data.answer, ensure_ascii=False)
    speaker1_memories_str = json.dumps(memory_data.speaker1_retrieval, ensure_ascii=False)
    speaker2_memories_str = json.dumps(memory_data.speaker2_retrieval, ensure_ascii=False)
    
    base_prompt = f"""
You are an evaluation assistant for the Memory Retrieval Stage.
Task:
Based strictly on speaker1_retrieval and speaker2_retrieval:
1. Determine whether the retrieved memories contain enough correct information to answer qa_question.
2. If sufficient â†’ is_sufficient = true (label = null).
3. If insufficient, determine the retrieval issue:
   - "3.1": Failed to recall correct information (missing the key facts)
   - "3.2": Unreasonable ranking (recalled irrelevant/common info while missing the most relevant facts)

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- speaker1_retrieval: {speaker1_memories_str}
- speaker2_retrieval: {speaker2_memories_str}
"""
    
    if other_opinions:
        opinions_text = "\n=== Other Models' Opinions ===\n"
        for op in other_opinions:
            opinions_text += f"""
Model ({op.model_name}):
- is_sufficient: {op.stage_passed}
- label: {op.label}
- reason: {op.reason}
"""
        base_prompt += opinions_text
        base_prompt += """
Consider the other models' opinions carefully. You may KEEP or CHANGE your judgment.
"""
    
    base_prompt += """
Output format:
{
  "is_sufficient": true/false,
  "label": "3.1" or "3.2" or null,
  "reason": "Detailed explanation"
}
"""
    return base_prompt


def generate_stage4_prompt(
    qa_data: QAData, 
    memory_data: MemoryData, 
    other_opinions: List[StageOpinion] = None
) -> str:
    """ç”Ÿæˆé˜¶æ®µ4çš„promptï¼ˆæ¨ç†ï¼‰"""
    qa_question_str = json.dumps(qa_data.question, ensure_ascii=False)
    qa_answer_str = json.dumps(qa_data.answer, ensure_ascii=False)
    qa_response_str = json.dumps(qa_data.response, ensure_ascii=False)
    speaker1_memories_str = json.dumps(memory_data.speaker1_retrieval, ensure_ascii=False)
    speaker2_memories_str = json.dumps(memory_data.speaker2_retrieval, ensure_ascii=False)
    
    base_prompt = f"""
You are an evaluation assistant for the Reasoning Stage.

Context:
All previous stages (extraction, update, retrieval) have passed, meaning the model had sufficient correct information.  
If qa_response still does not match qa_answer, the issue is a reasoning error.

Task:
Based on qa_question, qa_answer, qa_response, and the retrieved memories, classify the reasoning issue:
- "4.1": Correct memory entries were ignored (model overlooks correct memory entries present in retrieval)
- "4.2": Reasoning error (model invents details, over-specifies, or makes unsupported inferences)
- "4.3": Format or detail error (minor deviations such as missing qualifiers or altered phrasing that slightly change meaning)

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- speaker1_retrieval: {speaker1_memories_str}
- speaker2_retrieval: {speaker2_memories_str}
"""
    
    if other_opinions:
        opinions_text = "\n=== Other Models' Opinions ===\n"
        for op in other_opinions:
            opinions_text += f"""
Model ({op.model_name}):
- label: {op.label}
- reason: {op.reason}
"""
        base_prompt += opinions_text
        base_prompt += """
Consider the other models' opinions carefully. You may KEEP or CHANGE your judgment.
"""
    
    base_prompt += """
Output format:
{
  "label": "4.1" or "4.2" or "4.3",
  "reason": "Detailed explanation"
}
"""
    return base_prompt


# ============================================================================
# å•é˜¶æ®µè®¨è®ºå‡½æ•°
# ============================================================================

def discuss_stage(
    stage: DiagnosisStage,
    qa_data: QAData,
    memory_data: MemoryData,
    models: List[str],
    max_rounds: int = 3,
    config: Optional[DiagnosisConfig] = None
) -> StageDiscussionResult:
    """åœ¨æŸä¸ªé˜¶æ®µè¿›è¡Œå¤šæ¨¡å‹è®¨è®º
    
    Args:
        stage: å½“å‰è¯Šæ–­é˜¶æ®µ
        qa_data: QAæ•°æ®
        memory_data: è®°å¿†æ•°æ®
        models: å‚ä¸è®¨è®ºçš„æ¨¡å‹åˆ—è¡¨
        max_rounds: æœ€å¤§è®¨è®ºè½®æ¬¡
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageDiscussionResultå¯¹è±¡
    """
    stage_name_map = {
        DiagnosisStage.CONSISTENCY_CHECK: "ä¸€è‡´æ€§æ£€æŸ¥",
        DiagnosisStage.MEMORY_EXTRACTION: "è®°å¿†æå–",
        DiagnosisStage.MEMORY_UPDATE: "è®°å¿†æ›´æ–°",
        DiagnosisStage.MEMORY_RETRIEVAL: "è®°å¿†æ£€ç´¢",
        DiagnosisStage.REASONING: "æ¨ç†",
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ é˜¶æ®µ: {stage.value} - {stage_name_map.get(stage, stage.value)}")
    print(f"{'='*60}")
    
    # é€‰æ‹©å¯¹åº”é˜¶æ®µçš„promptç”Ÿæˆå‡½æ•°
    prompt_generators = {
        DiagnosisStage.CONSISTENCY_CHECK: lambda ops: generate_stage0_prompt(qa_data, ops),
        DiagnosisStage.MEMORY_EXTRACTION: lambda ops: generate_stage1_prompt(qa_data, memory_data, ops),
        DiagnosisStage.MEMORY_UPDATE: lambda ops: generate_stage2_prompt(qa_data, memory_data, ops),
        DiagnosisStage.MEMORY_RETRIEVAL: lambda ops: generate_stage3_prompt(qa_data, memory_data, ops),
        DiagnosisStage.REASONING: lambda ops: generate_stage4_prompt(qa_data, memory_data, ops),
    }
    
    generate_prompt = prompt_generators[stage]
    discussion_history = []
    current_opinions = []
    
    # ç¬¬ä¸€è½®ï¼šç‹¬ç«‹åˆ¤æ–­
    print(f"\nğŸ”„ ç¬¬ 1 è½®ï¼šç‹¬ç«‹åˆ¤æ–­")
    
    for model in models:
        print(f"   ğŸ¤– {model} æ­£åœ¨åˆ†æ...")
        prompt = generate_prompt(None)  # ç¬¬ä¸€è½®æ²¡æœ‰å…¶ä»–æ„è§
        
        try:
            result = call_llm_api(clean_prompt(prompt), model, config)
            
            # æ ¹æ®é˜¶æ®µè§£æç»“æœ
            if stage == DiagnosisStage.CONSISTENCY_CHECK:
                stage_passed = result.get("is_consistent", False)
                label = None
            elif stage == DiagnosisStage.REASONING:
                stage_passed = False  # é˜¶æ®µ4æ€»æ˜¯è¿”å›ä¸€ä¸ªlabel
                label = result.get("label")
            else:
                stage_passed = result.get("is_sufficient", False)
                label = None if stage_passed else result.get("label")
            
            reason = result.get("reason", "")
            
            opinion = StageOpinion(
                model_name=model,
                stage_passed=stage_passed,
                label=label,
                reason=reason,
                round_num=1
            )
            current_opinions.append(opinion)
            
            print(f"      âœ“ {model}: passed={stage_passed}, label={label}")
            
        except Exception as e:
            logging.error(f"æ¨¡å‹ {model} é˜¶æ®µ {stage.value} åˆ†æå¤±è´¥: {str(e)}")
            opinion = StageOpinion(
                model_name=model,
                stage_passed=False,
                label=None,
                reason=f"åˆ†æå¤±è´¥: {str(e)}",
                round_num=1
            )
            current_opinions.append(opinion)
    
    # è®°å½•ç¬¬ä¸€è½®å†å²
    discussion_history.append({
        "round": 1,
        "opinions": [
            {"model": op.model_name, "passed": op.stage_passed, "label": op.label, "reason": op.reason}
            for op in current_opinions
        ]
    })
    
    # æ£€æŸ¥æ˜¯å¦è¾¾æˆå…±è¯†
    def check_consensus(opinions: List[StageOpinion]) -> Tuple[bool, Optional[bool], Optional[str]]:
        """æ£€æŸ¥æ˜¯å¦è¾¾æˆå…±è¯†ï¼Œè¿”å› (æ˜¯å¦å…±è¯†, å…±è¯†çš„passedå€¼, å…±è¯†çš„label)"""
        if stage == DiagnosisStage.REASONING:
            # é˜¶æ®µ4åªçœ‹label
            labels = [op.label for op in opinions]
            if len(set(labels)) == 1:
                return True, False, labels[0]
            return False, None, None
        else:
            # å…¶ä»–é˜¶æ®µå…ˆçœ‹passedï¼Œå¦‚æœéƒ½ä¸é€šè¿‡å†çœ‹label
            passed_values = [op.stage_passed for op in opinions]
            if len(set(passed_values)) == 1:
                if passed_values[0]:  # éƒ½é€šè¿‡
                    return True, True, None
                else:  # éƒ½ä¸é€šè¿‡ï¼Œæ£€æŸ¥label
                    labels = [op.label for op in opinions]
                    if len(set(labels)) == 1:
                        return True, False, labels[0]
            return False, None, None
    
    consensus, consensus_passed, consensus_label = check_consensus(current_opinions)
    
    if consensus:
        print(f"\nğŸ‰ ç¬¬ 1 è½®å³è¾¾æˆå…±è¯†ï¼passed={consensus_passed}, label={consensus_label}")
        return StageDiscussionResult(
            stage=stage,
            consensus_reached=True,
            final_passed=consensus_passed,
            final_label=consensus_label,
            final_reason=current_opinions[0].reason,
            total_rounds=1,
            discussion_history=discussion_history
        )
    
    # åç»­è½®æ¬¡ï¼šè®¨è®º
    for round_num in range(2, max_rounds + 1):
        print(f"\nğŸ”„ ç¬¬ {round_num} è½®ï¼šè®¨è®º")
        
        new_opinions = []
        
        for model in models:
            # è·å–å…¶ä»–æ¨¡å‹çš„æ„è§
            other_opinions = [op for op in current_opinions if op.model_name != model]
            current_model_opinion = next(op for op in current_opinions if op.model_name == model)
            
            print(f"   ğŸ¤– {model} æ­£åœ¨å‚è€ƒå…¶ä»–æ„è§...")
            prompt = generate_prompt(other_opinions)
            
            try:
                result = call_llm_api(clean_prompt(prompt), model, config)
                
                # æ ¹æ®é˜¶æ®µè§£æç»“æœ
                if stage == DiagnosisStage.CONSISTENCY_CHECK:
                    stage_passed = result.get("is_consistent", False)
                    label = None
                elif stage == DiagnosisStage.REASONING:
                    stage_passed = False
                    label = result.get("label")
                else:
                    stage_passed = result.get("is_sufficient", False)
                    label = None if stage_passed else result.get("label")
                
                reason = result.get("reason", "")
                
                # è®°å½•æ˜¯å¦æ”¹å˜äº†æ„è§
                changed_from_passed = None
                changed_from_label = None
                if stage_passed != current_model_opinion.stage_passed:
                    changed_from_passed = current_model_opinion.stage_passed
                    print(f"      â†ªï¸ {model} ä¿®æ”¹äº†åˆ¤æ–­: passed {current_model_opinion.stage_passed} â†’ {stage_passed}")
                elif label != current_model_opinion.label:
                    changed_from_label = current_model_opinion.label
                    print(f"      â†ªï¸ {model} ä¿®æ”¹äº†æ ‡ç­¾: {current_model_opinion.label} â†’ {label}")
                else:
                    print(f"      âœ“ {model} ä¿æŒåˆ¤æ–­: passed={stage_passed}, label={label}")
                
                opinion = StageOpinion(
                    model_name=model,
                    stage_passed=stage_passed,
                    label=label,
                    reason=reason,
                    round_num=round_num,
                    changed_from_passed=changed_from_passed,
                    changed_from_label=changed_from_label
                )
                new_opinions.append(opinion)
                
            except Exception as e:
                logging.error(f"æ¨¡å‹ {model} è®¨è®ºå¤±è´¥: {str(e)}")
                # ä¿æŒåŸæ„è§
                opinion = StageOpinion(
                    model_name=model,
                    stage_passed=current_model_opinion.stage_passed,
                    label=current_model_opinion.label,
                    reason=f"è®¨è®ºå¤±è´¥ï¼Œä¿æŒåŸæ„è§: {str(e)}",
                    round_num=round_num
                )
                new_opinions.append(opinion)
        
        current_opinions = new_opinions
        
        # è®°å½•æœ¬è½®å†å²
        discussion_history.append({
            "round": round_num,
            "opinions": [
                {
                    "model": op.model_name, 
                    "passed": op.stage_passed, 
                    "label": op.label, 
                    "reason": op.reason,
                    "changed_from_passed": op.changed_from_passed,
                    "changed_from_label": op.changed_from_label
                }
                for op in current_opinions
            ]
        })
        
        # æ£€æŸ¥å…±è¯†
        consensus, consensus_passed, consensus_label = check_consensus(current_opinions)
        
        if consensus:
            print(f"\nğŸ‰ ç¬¬ {round_num} è½®è¾¾æˆå…±è¯†ï¼passed={consensus_passed}, label={consensus_label}")
            return StageDiscussionResult(
                stage=stage,
                consensus_reached=True,
                final_passed=consensus_passed,
                final_label=consensus_label,
                final_reason=current_opinions[0].reason,
                total_rounds=round_num,
                discussion_history=discussion_history
            )
    
    # æœªè¾¾æˆå…±è¯†ï¼ŒæŠ•ç¥¨å†³å®š
    print(f"\nâš ï¸ {max_rounds} è½®åæœªè¾¾æˆå…±è¯†ï¼Œè¿›è¡ŒæŠ•ç¥¨")
    
    # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ gpt-5 çš„ç»“æœ
    def get_gpt5_opinion(opinions: List[StageOpinion]) -> Optional[StageOpinion]:
        """è·å– gpt-5 çš„æ„è§"""
        for op in opinions:
            if op.model_name == "gpt-5":
                return op
        return None
    
    if stage == DiagnosisStage.REASONING:
        # é˜¶æ®µ4åªæŠ•ç¥¨label
        labels = [op.label for op in current_opinions]
        label_counter = Counter(labels)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼ˆ1:1:1ï¼‰
        all_different = len(label_counter) == len(current_opinions) and len(current_opinions) > 1
        
        if all_different:
            # æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼Œä½¿ç”¨ gpt-5 çš„ç»“æœ
            gpt5_op = get_gpt5_opinion(current_opinions)
            if gpt5_op:
                final_label = gpt5_op.label
                print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: {dict(label_counter)}")
                print(f"âš ï¸ æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼Œä½¿ç”¨ gpt-5 çš„ç»“æœ")
                print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label}")
            else:
                final_label = label_counter.most_common(1)[0][0]
                print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: {dict(label_counter)}")
                print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label}")
        else:
            final_label = label_counter.most_common(1)[0][0]
            print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: {dict(label_counter)}")
            print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label} (å¾—ç¥¨æœ€å¤š)")
        final_passed = False
    else:
        # å…ˆæŠ•ç¥¨passed
        passed_values = [op.stage_passed for op in current_opinions]
        passed_counter = Counter(passed_values)
        final_passed = passed_counter.most_common(1)[0][0]
        
        if final_passed:
            final_label = None
            print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: passed={dict(passed_counter)}")
            print(f"ğŸ† é˜¶æ®µé€šè¿‡")
        else:
            # ä¸é€šè¿‡æ—¶æŠ•ç¥¨label
            labels = [op.label for op in current_opinions if not op.stage_passed]
            if labels:
                label_counter = Counter(labels)
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼ˆ1:1:1ï¼‰
                all_different = len(label_counter) == len(labels) and len(labels) > 1
                
                if all_different:
                    # æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼Œä½¿ç”¨ gpt-5 çš„ç»“æœ
                    gpt5_op = get_gpt5_opinion([op for op in current_opinions if not op.stage_passed])
                    if gpt5_op:
                        final_label = gpt5_op.label
                        print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: passed={dict(passed_counter)}, labels={dict(label_counter)}")
                        print(f"âš ï¸ æ‰€æœ‰ç»“æœéƒ½ä¸åŒï¼Œä½¿ç”¨ gpt-5 çš„ç»“æœ")
                        print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label}")
                    else:
                        final_label = label_counter.most_common(1)[0][0]
                        print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: passed={dict(passed_counter)}, labels={dict(label_counter)}")
                        print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label}")
                else:
                    final_label = label_counter.most_common(1)[0][0]
                    print(f"ğŸ“Š æŠ•ç¥¨ç»“æœ: passed={dict(passed_counter)}, labels={dict(label_counter)}")
                    print(f"ğŸ† é€‰æ‹©æ ‡ç­¾: {final_label} (å¾—ç¥¨æœ€å¤š)")
            else:
                final_label = None
    
    # è·å–å¯¹åº”æ„è§çš„reason
    final_opinion = None
    for op in current_opinions:
        if stage == DiagnosisStage.REASONING:
            if op.label == final_label:
                final_opinion = op
                break
        else:
            if op.stage_passed == final_passed and op.label == final_label:
                final_opinion = op
                break
    
    return StageDiscussionResult(
        stage=stage,
        consensus_reached=False,
        final_passed=final_passed,
        final_label=final_label,
        final_reason=final_opinion.reason if final_opinion else "",
        total_rounds=max_rounds,
        discussion_history=discussion_history
    )


# ============================================================================
# å®Œæ•´è¯Šæ–­è®¨è®ºå‡½æ•°
# ============================================================================

def analyze_qa_pair_with_discussion(
    qa_question: str,
    qa_answer: str,
    qa_response: str,
    memories1: List[dict],
    memories2: List[dict],
    speaker1_memories: List[Dict],
    speaker2_memories: List[Dict],
    models: List[str] = None,
    max_rounds: int = 3,
    config: Optional[DiagnosisConfig] = None
) -> Dict:
    """ä½¿ç”¨å¤šæ¨¡å‹åˆ†é˜¶æ®µè®¨è®ºæœºåˆ¶åˆ†æQAå¯¹
    
    æŒ‰é¡ºåºæ‰§è¡Œå„é˜¶æ®µè®¨è®ºï¼Œæ¯ä¸ªé˜¶æ®µä¸‰æ¨¡å‹å…ˆç‹¬ç«‹åˆ¤æ–­å†è®¨è®ºè¾¾æˆå…±è¯†
    
    Args:
        qa_question: é—®é¢˜æ–‡æœ¬
        qa_answer: å‚è€ƒç­”æ¡ˆ
        qa_response: æ¨¡å‹å›ç­”
        memories1: person1çš„è®°å¿†æ•°æ®
        memories2: person2çš„è®°å¿†æ•°æ®
        speaker1_memories: speaker1çš„æ£€ç´¢è®°å¿†
        speaker2_memories: speaker2çš„æ£€ç´¢è®°å¿†
        models: å‚ä¸è®¨è®ºçš„æ¨¡å‹åˆ—è¡¨
        max_rounds: æ¯ä¸ªé˜¶æ®µæœ€å¤§è®¨è®ºè½®æ¬¡
        config: è¯Šæ–­é…ç½®
        
    Returns:
        åŒ…å«è®¨è®ºç»“æœçš„å­—å…¸
    """
    if models is None:
        models = ["deepseek", "gpt-4.1", "gpt-5"]
    
    print(f"\n{'='*70}")
    print(f"ğŸ—£ï¸  å¤šæ¨¡å‹åˆ†é˜¶æ®µè®¨è®ºè¯Šæ–­")
    print(f"{'='*70}")
    print(f"ğŸ“ é—®é¢˜: {qa_question}")
    print(f"ğŸ¤– å‚ä¸æ¨¡å‹: {', '.join(models)}")
    print(f"ğŸ”„ æ¯é˜¶æ®µæœ€å¤§è½®æ¬¡: {max_rounds}")
    print(f"{'='*70}")
    
    # åˆ›å»ºæ•°æ®å¯¹è±¡
    qa_data = QAData(
        question=qa_question,
        answer=qa_answer,
        response=qa_response
    )
    
    memory_data = MemoryData(
        person1_memories=memories1,
        person2_memories=memories2,
        speaker1_retrieval=speaker1_memories,
        speaker2_retrieval=speaker2_memories
    )
    
    stage_results = {}
    
    # ========== é˜¶æ®µ0ï¼šä¸€è‡´æ€§æ£€æŸ¥ ==========
    stage0_result = discuss_stage(
        DiagnosisStage.CONSISTENCY_CHECK,
        qa_data, memory_data, models, max_rounds, config
    )
    stage_results["0_consistency_check"] = stage0_result
    
    if stage0_result.final_passed:
        # ä¸€è‡´ï¼Œç›´æ¥è¿”å›
        print(f"\n{'='*70}")
        print(f"âœ… è¯Šæ–­å®Œæˆï¼šå›ç­”ä¸ç­”æ¡ˆä¸€è‡´")
        print(f"{'='*70}\n")
        
        return {
            "label": None,
            "reason": stage0_result.final_reason,
            "stage": DiagnosisStage.CONSISTENCY_CHECK.value,
            "consensus_reached": stage0_result.consensus_reached,
            "total_stage_rounds": {"0_consistency_check": stage0_result.total_rounds},
            "stage_results": _serialize_stage_results(stage_results)
        }
    
    # ========== é˜¶æ®µ1ï¼šè®°å¿†æå– ==========
    stage1_result = discuss_stage(
        DiagnosisStage.MEMORY_EXTRACTION,
        qa_data, memory_data, models, max_rounds, config
    )
    stage_results["1_memory_extraction"] = stage1_result
    
    if not stage1_result.final_passed:
        print(f"\n{'='*70}")
        print(f"âŒ è¯Šæ–­å®Œæˆï¼šè®°å¿†æå–é˜¶æ®µå‘ç°é—®é¢˜")
        print(f"   æ ‡ç­¾: {stage1_result.final_label}")
        print(f"{'='*70}\n")
        
        return {
            "label": stage1_result.final_label,
            "reason": stage1_result.final_reason,
            "stage": DiagnosisStage.MEMORY_EXTRACTION.value,
            "consensus_reached": stage1_result.consensus_reached,
            "total_stage_rounds": {
                "0_consistency_check": stage0_result.total_rounds,
                "1_memory_extraction": stage1_result.total_rounds
            },
            "stage_results": _serialize_stage_results(stage_results)
        }
    
    # ========== é˜¶æ®µ2ï¼šè®°å¿†æ›´æ–° ==========
    stage2_result = discuss_stage(
        DiagnosisStage.MEMORY_UPDATE,
        qa_data, memory_data, models, max_rounds, config
    )
    stage_results["2_memory_update"] = stage2_result
    
    if not stage2_result.final_passed:
        print(f"\n{'='*70}")
        print(f"âŒ è¯Šæ–­å®Œæˆï¼šè®°å¿†æ›´æ–°é˜¶æ®µå‘ç°é—®é¢˜")
        print(f"   æ ‡ç­¾: {stage2_result.final_label}")
        print(f"{'='*70}\n")
        
        return {
            "label": stage2_result.final_label,
            "reason": stage2_result.final_reason,
            "stage": DiagnosisStage.MEMORY_UPDATE.value,
            "consensus_reached": stage2_result.consensus_reached,
            "total_stage_rounds": {
                "0_consistency_check": stage0_result.total_rounds,
                "1_memory_extraction": stage1_result.total_rounds,
                "2_memory_update": stage2_result.total_rounds
            },
            "stage_results": _serialize_stage_results(stage_results)
        }
    
    # ========== é˜¶æ®µ3ï¼šè®°å¿†æ£€ç´¢ ==========
    stage3_result = discuss_stage(
        DiagnosisStage.MEMORY_RETRIEVAL,
        qa_data, memory_data, models, max_rounds, config
    )
    stage_results["3_memory_retrieval"] = stage3_result
    
    if not stage3_result.final_passed:
        print(f"\n{'='*70}")
        print(f"âŒ è¯Šæ–­å®Œæˆï¼šè®°å¿†æ£€ç´¢é˜¶æ®µå‘ç°é—®é¢˜")
        print(f"   æ ‡ç­¾: {stage3_result.final_label}")
        print(f"{'='*70}\n")
        
        return {
            "label": stage3_result.final_label,
            "reason": stage3_result.final_reason,
            "stage": DiagnosisStage.MEMORY_RETRIEVAL.value,
            "consensus_reached": stage3_result.consensus_reached,
            "total_stage_rounds": {
                "0_consistency_check": stage0_result.total_rounds,
                "1_memory_extraction": stage1_result.total_rounds,
                "2_memory_update": stage2_result.total_rounds,
                "3_memory_retrieval": stage3_result.total_rounds
            },
            "stage_results": _serialize_stage_results(stage_results)
        }
    
    # ========== é˜¶æ®µ4ï¼šæ¨ç† ==========
    stage4_result = discuss_stage(
        DiagnosisStage.REASONING,
        qa_data, memory_data, models, max_rounds, config
    )
    stage_results["4_reasoning"] = stage4_result
    
    print(f"\n{'='*70}")
    print(f"âŒ è¯Šæ–­å®Œæˆï¼šæ¨ç†é˜¶æ®µå‘ç°é—®é¢˜")
    print(f"   æ ‡ç­¾: {stage4_result.final_label}")
    print(f"{'='*70}\n")
    
    return {
        "label": stage4_result.final_label,
        "reason": stage4_result.final_reason,
        "stage": DiagnosisStage.REASONING.value,
        "consensus_reached": stage4_result.consensus_reached,
        "total_stage_rounds": {
            "0_consistency_check": stage0_result.total_rounds,
            "1_memory_extraction": stage1_result.total_rounds,
            "2_memory_update": stage2_result.total_rounds,
            "3_memory_retrieval": stage3_result.total_rounds,
            "4_reasoning": stage4_result.total_rounds
        },
        "stage_results": _serialize_stage_results(stage_results)
    }


def _serialize_stage_results(stage_results: Dict[str, StageDiscussionResult]) -> Dict:
    """å°†é˜¶æ®µç»“æœåºåˆ—åŒ–ä¸ºå¯JSONåŒ–çš„æ ¼å¼"""
    result = {}
    for stage_name, stage_data in stage_results.items():
        result[stage_name] = {
            "consensus_reached": stage_data.consensus_reached,
            "final_passed": stage_data.final_passed,
            "final_label": stage_data.final_label,
            "final_reason": stage_data.final_reason,
            "total_rounds": stage_data.total_rounds,
            "discussion_history": stage_data.discussion_history
        }
    return result


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»ç¨‹åºå…¥å£å‡½æ•°
    
    æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼š
        python run_diagnosis_discussion.py [options]
        
    å‚æ•°è¯´æ˜ï¼š
        --max-rounds N: æ¯ä¸ªé˜¶æ®µæœ€å¤§è®¨è®ºè½®æ¬¡ï¼Œé»˜è®¤ï¼š3
        --models: å‚ä¸è®¨è®ºçš„æ¨¡å‹ï¼Œé»˜è®¤ï¼šdeepseek gpt-4.1 gpt-5
        -i, --input: è¾“å…¥æ–‡ä»¶è·¯å¾„
        -o, --output-dir: è¾“å‡ºç›®å½•è·¯å¾„
        -f, --output-file: è¾“å‡ºæ–‡ä»¶å
        
    ç¤ºä¾‹ï¼š
        python run_diagnosis_discussion.py --max-rounds 3
        python run_diagnosis_discussion.py --models deepseek gpt-4.1 gpt-5
        python run_diagnosis_discussion.py -i data/input.json -o results/
    """
    parser = argparse.ArgumentParser(
        description="è®°å¿†è¯Šæ–­ç³»ç»Ÿ - å¤šæ¨¡å‹åˆ†é˜¶æ®µè®¨è®ºç‰ˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--max-rounds",
        type=int,
        default=3,
        help="æ¯ä¸ªé˜¶æ®µæœ€å¤§è®¨è®ºè½®æ¬¡ (é»˜è®¤: 3)"
    )
    
    parser.add_argument(
        "--models",
        nargs="+",
        default=["deepseek", "gpt-4.1", "gpt-5"],
        help="å‚ä¸è®¨è®ºçš„æ¨¡å‹åˆ—è¡¨ (é»˜è®¤: deepseek gpt-4.1 gpt-5)"
    )
    
    parser.add_argument(
        "-i", "--input",
        type=str,
        default="data/input/mem0_mem/sample/sampled_qa_50.json",
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "-o", "--output-dir",
        type=str,
        default="data/output/llm_annotation_discussion",
        help="è¾“å‡ºç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "-f", "--output-file",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)"
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ è®°å¿†è¯Šæ–­ç³»ç»Ÿ - å¤šæ¨¡å‹åˆ†é˜¶æ®µè®¨è®ºç‰ˆ å¯åŠ¨")
    print("="*70)
    print(f"ğŸ¤– å‚ä¸æ¨¡å‹: {', '.join(args.models)}")
    print(f"ğŸ”„ æ¯é˜¶æ®µæœ€å¤§è®¨è®ºè½®æ¬¡: {args.max_rounds}")
    print(f"âš™ï¸  é…ç½®: {DiagnosisConfig()}")
    print("="*70 + "\n")
    
    input_file = args.input
    input_basename = os.path.splitext(os.path.basename(input_file))[0]
    input_identifier = input_basename.replace(" ", "_").replace("(", "").replace(")", "")
    
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if args.output_file:
        output_filename = args.output_file
    else:
        models_str = "_".join([m.replace("-", "").replace(".", "") for m in args.models])
        output_filename = f"{input_identifier}_discussion_{args.max_rounds}rounds_{models_str}_{timestamp}.json"
    
    output_file = os.path.join(output_dir, output_filename)
    
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}\n")
    
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    try:
        data = load_json_file(input_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªä¼šè¯\n")
    except Exception as e:
        logging.error(f"åŠ è½½è¾“å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")
        print(f"âŒ é”™è¯¯: æ— æ³•è§£æè¾“å…¥æ–‡ä»¶: {str(e)}")
        return
    
    # åŠ è½½å·²å¤„ç†çš„ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    results = []
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                results = json.load(f)
                logging.info(f"å·²åŠ è½½ {len(results)} æ¡å†å²ç»“æœ")
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logging.warning(f"åŠ è½½å†å²ç»“æœå¤±è´¥: {str(e)}ï¼Œå°†ä»å¤´å¼€å§‹")
            results = []
    
    processed_items = {item["conv_id_question_id"] for item in results}
    
    try:
        total_convs = len(data)
        print(f"ğŸ“Š å¼€å§‹å¤„ç†ï¼Œå…±æœ‰ {total_convs} ä¸ªä¼šè¯éœ€è¦åˆ†æ\n")
        
        for conv_idx, (conv_id, qa_list) in enumerate(data.items(), 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ å¤„ç†ä¼šè¯ {conv_id} ({conv_idx}/{total_convs})")
            print(f"{'='*70}\n")
            
            for qa_idx, qa_item in enumerate(qa_list, 1):
                # ä¼˜å…ˆä½¿ç”¨ original_source ä¸­çš„ç´¢å¼•ï¼Œå¦åˆ™ä½¿ç”¨åˆ—è¡¨ç´¢å¼•
                original_source = qa_item.get("original_source", {})
                if original_source and original_source.get("qa_index") is not None:
                    item_id = f"{conv_id}_{original_source['qa_index']}"
                else:
                    item_id = f"{conv_id}_{qa_idx-1}"
                
                if item_id in processed_items:
                    print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„é—®é¢˜: {item_id}\n")
                    continue
                
                print(f"ğŸ” å¼€å§‹å¤„ç†é—®é¢˜ {qa_idx}/{len(qa_list)}: {item_id}")
                
                try:
                    p1 = qa_item.get("person1", {})
                    p2 = qa_item.get("person2", {})
                    memories1 = p1.get("memories", [])
                    memories2 = p2.get("memories", [])
                    
                    analysis = analyze_qa_pair_with_discussion(
                        qa_question=qa_item["qa_question"],
                        qa_answer=qa_item["qa_answer"],
                        qa_response=qa_item["qa_response"],
                        memories1=memories1,
                        memories2=memories2,
                        speaker1_memories=qa_item.get("speaker_1_memories", []),
                        speaker2_memories=qa_item.get("speaker_2_memories", []),
                        models=args.models,
                        max_rounds=args.max_rounds
                    )
                    
                    # è·å–åŸå§‹ä½ç½®ä¿¡æ¯
                    original_source = qa_item.get("original_source", {})
                    if original_source:
                        original_id = f"{original_source.get('file', '')}_{original_source.get('key', '')}_{original_source.get('qa_index', '')}"
                    else:
                        original_id = item_id
                    
                    result = {
                        "conv_id_question_id": item_id,
                        "original_id": original_id,
                        "original_source": original_source,
                        "qa_question": qa_item["qa_question"],
                        "qa_answer": qa_item["qa_answer"],
                        "qa_response": qa_item["qa_response"],
                        "qa_category": qa_item.get("qa_category", ""),
                        "label": analysis["label"],
                        "reason": analysis["reason"],
                        "stage": analysis["stage"],
                        "diagnosis_mode": f"discussion_{args.max_rounds}rounds_per_stage",
                        "consensus_reached": analysis["consensus_reached"],
                        "total_stage_rounds": analysis["total_stage_rounds"],
                        "stage_results": analysis["stage_results"]
                    }
                    
                    results.append(result)
                    
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    
                    print(f"âœ… é—®é¢˜ {item_id} å¤„ç†å®Œæˆå¹¶å·²ä¿å­˜\n")
                    
                except Exception as e:
                    logging.error(f"å¤„ç†é—®é¢˜ {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    print(f"âŒ å¤„ç†é—®é¢˜ {item_id} å¤±è´¥: {str(e)}\n")
                    continue
                    
    except KeyboardInterrupt:
        print("\nâš ï¸  å¤„ç†å·²ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜...\n")
    except Exception as e:
        logging.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\n")
    finally:
        if results:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            consensus_count = sum(1 for r in results if r.get("consensus_reached", False))
            
            print("\n" + "="*70)
            print("ğŸ‰ å¤„ç†å®Œæˆ")
            print("="*70)
            print(f"âœ… å…±å¤„ç† {len(results)} ä¸ªé—®é¢˜")
            print(f"ğŸ¤ è¾¾æˆå…±è¯†: {consensus_count}/{len(results)} ({100*consensus_count/len(results):.1f}%)")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            print("="*70 + "\n")


if __name__ == "__main__":
    main()

