"""
è®°å¿†è¯Šæ–­ç³»ç»Ÿ - åˆ†é˜¶æ®µè¯Šæ–­QAå¯¹ä¸­çš„é—®é¢˜

è¯¥æ¨¡å—æä¾›äº†ä¸€ä¸ªåˆ†é˜¶æ®µçš„è¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«è®°å¿†ç³»ç»Ÿä¸­çš„é—®é¢˜ç±»å‹ï¼š
- é˜¶æ®µ0: ä¸€è‡´æ€§æ£€æŸ¥
- é˜¶æ®µ1: è®°å¿†æå–è¯Šæ–­
- é˜¶æ®µ2: è®°å¿†æ›´æ–°è¯Šæ–­
- é˜¶æ®µ3: è®°å¿†æ£€ç´¢è¯Šæ–­
- é˜¶æ®µ4: æ¨ç†è¯Šæ–­
"""

# æ ‡å‡†åº“å¯¼å…¥
import json
import logging
import os
import re
import sys
import time
import warnings
from collections import Counter
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from dotenv import load_dotenv
from requests.exceptions import RequestException, Timeout

# æ³¨æ„ï¼šAI API ç›¸å…³çš„å¯¼å…¥å·²ç§»åˆ°å„è‡ªçš„å‡½æ•°ä¸­ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
# è¿™æ ·å³ä½¿æŸäº›åº“æœªå®‰è£…ï¼Œä¹Ÿä¸ä¼šå½±å“å…¶ä»–åŠŸèƒ½çš„ä½¿ç”¨

# ============================================================================
# é…ç½®å’Œåˆå§‹åŒ–
# ============================================================================

# æŠ‘åˆ¶gRPCè­¦å‘Š
logging.getLogger('grpc').setLevel(logging.ERROR)
warnings.filterwarnings('ignore', module='grpc')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
os.environ['GRPC_ALTS_CREDENTIALS_ENVIRONMENT_OVERRIDE'] = '1'


# ============================================================================
# æšä¸¾å’Œå¸¸é‡å®šä¹‰
# ============================================================================

class ModelType(str, Enum):
    """æ”¯æŒçš„LLMæ¨¡å‹ç±»å‹"""
    QWEN = "qwen"
    DEEPSEEK = "deepseek"
    GPT_4_1 = "gpt-4.1"
    GPT_5 = "gpt-5"
    GEMINI = "gemini-2.5-pro"


class DiagnosisStage(str, Enum):
    """è¯Šæ–­é˜¶æ®µæšä¸¾"""
    CONSISTENCY_CHECK = "0_consistency_check"
    MEMORY_EXTRACTION = "1_memory_extraction"
    MEMORY_UPDATE = "2_memory_update"
    MEMORY_RETRIEVAL = "3_memory_retrieval"
    REASONING = "4_reasoning"
    ERROR = "error"


# ============================================================================
# é…ç½®ç±»
# ============================================================================

@dataclass
class APIConfig:
    """APIé…ç½®"""
    dashscope_api_key: str = field(default_factory=lambda: os.getenv("DASHSCOPE_API_KEY", ""))
    deepseek_api_key: str = field(default_factory=lambda: os.getenv("DEEPSEEK_API_KEY", ""))
    deepseek_api_url: str = field(default_factory=lambda: os.getenv("DEEPSEEK_API_URL", ""))
    openai_api_key: str = field(default_factory=lambda: os.getenv("OPENAI_API_KEY", ""))
    gemini_api_key: str = field(default_factory=lambda: os.getenv("GEMINI_API_KEY", ""))
    gemini_url: str = field(default_factory=lambda: os.getenv("GEMINI_URL", ""))


@dataclass
class DiagnosisConfig:
    """è¯Šæ–­é…ç½®"""
    model: ModelType = ModelType.DEEPSEEK
    max_retries: int = 3
    retry_delay: int = 5
    temperature: float = 0.1
    timeout: int = 30


# ============================================================================
# æ•°æ®ç±»
# ============================================================================

@dataclass
class QAData:
    """QAæ•°æ®å°è£…"""
    question: str
    answer: str
    response: str
    category: str = ""
    
    def to_json_str(self, field_name: str) -> str:
        """å°†æŒ‡å®šå­—æ®µè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        value = getattr(self, field_name.replace("qa_", ""))
        return json.dumps(value, ensure_ascii=False)


@dataclass
class MemoryData:
    """è®°å¿†æ•°æ®å°è£…"""
    person1_memories: List[dict] = field(default_factory=list)
    person2_memories: List[dict] = field(default_factory=list)
    speaker1_retrieval: List[Dict] = field(default_factory=list)
    speaker2_retrieval: List[Dict] = field(default_factory=list)
    
    def to_json_str(self, field_name: str, exclude_keys: Optional[List[str]] = None) -> str:
        """å°†æŒ‡å®šå­—æ®µè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        
        Args:
            field_name: å­—æ®µåç§°
            exclude_keys: éœ€è¦ä»æ¯ä¸ªè®°å¿†é¡¹ä¸­æ’é™¤çš„é”®åˆ—è¡¨
            
        Returns:
            JSONå­—ç¬¦ä¸²
        """
        value = getattr(self, field_name)
        
        # å¦‚æœéœ€è¦æ’é™¤æŸäº›é”®ï¼Œåˆ™è¿‡æ»¤æ•°æ®
        if exclude_keys and isinstance(value, list):
            filtered_value = []
            for item in value:
                if isinstance(item, dict):
                    # åˆ›å»ºä¸€ä¸ªæ–°å­—å…¸ï¼Œæ’é™¤æŒ‡å®šçš„é”®
                    filtered_item = {k: v for k, v in item.items() if k not in exclude_keys}
                    filtered_value.append(filtered_item)
                else:
                    filtered_value.append(item)
            return json.dumps(filtered_value, ensure_ascii=False)
        
        return json.dumps(value, ensure_ascii=False)


@dataclass
class StageResult:
    """é˜¶æ®µè¯Šæ–­ç»“æœ"""
    stage_passed: bool
    label: Optional[str]
    reason: str
    stage: Optional[DiagnosisStage] = None


@dataclass
class DiagnosisResult:
    """å®Œæ•´è¯Šæ–­ç»“æœ"""
    label: Optional[str]
    reason: str
    stage: DiagnosisStage
    used_model: Optional[str] = None
    voting_details: Optional[Dict] = None


# åˆå§‹åŒ–å…¨å±€é…ç½®
API_CONFIG = APIConfig()
# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def load_json_file(file_path: str) -> Dict:
    """åŠ è½½JSONæ–‡ä»¶
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„å­—å…¸å¯¹è±¡
    """
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


def clean_prompt(prompt: str) -> str:
    """æ¸…ç†promptä¸­çš„ç‰¹æ®Šå­—ç¬¦
    
    Args:
        prompt: åŸå§‹promptæ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„promptæ–‡æœ¬
    """
    return re.sub(r"[\u200b\u200c\u200d\ufeff\u202a-\u202e]", "", prompt)


def extract_json_from_response(response_text: str) -> Dict:
    """ä»å“åº”æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡
    
    Args:
        response_text: LLMå“åº”æ–‡æœ¬
        
    Returns:
        è§£æåçš„JSONå¯¹è±¡
        
    Raises:
        Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    response_text = response_text.strip()
    start = response_text.find("{")
    end = response_text.rfind("}") + 1
    
    if start != -1 and end != 0:
        response_text = response_text[start:end]
    
    return json.loads(response_text)


# ============================================================================
# LLM APIè°ƒç”¨å‡½æ•°
# ============================================================================

def call_deepseek_api(prompt: str, temperature: float = 0.1) -> Dict:
    """è°ƒç”¨DeepSeek API
    
    Args:
        prompt: è¾“å…¥prompt
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        æ ‡å‡†åŒ–çš„å“åº”å­—å…¸ {"output": {"text": "..."}}
        
    Raises:
        Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        from openai import OpenAI
    except ImportError:
        raise Exception("è¯·å®‰è£… openai åº“: pip install openai")
    
    client = OpenAI(
        api_key=API_CONFIG.deepseek_api_key,
        base_url=API_CONFIG.deepseek_api_url
    )
    
    try:
        kwargs = {
            "model": "deepseek-reasoner",
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "temperature": temperature,
        }
        response = client.chat.completions.create(**kwargs)
        return {"output": {"text": response.choices[0].message.content}}
    except Exception as e:
        # æ‰“å°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        try:
            resp = getattr(e, "response", None)
            if resp is not None:
                logging.error(f"DeepSeek response status: {getattr(resp, 'status_code', None)}")
                logging.error(f"DeepSeek response body: {getattr(resp, 'text', None)}")
        except Exception:
            pass
        
        logging.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {repr(e)}")
        
        # å¦‚æœæ˜¯temperatureå‚æ•°é—®é¢˜ï¼Œå°è¯•ä¸å¸¦temperatureé‡è¯•
        err_text = repr(e).lower()
        if "temperature" in err_text or "unsupported" in err_text:
            try:
                kwargs = {
                    "model": "deepseek-reasoner",
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                }
                response = client.chat.completions.create(**kwargs)
                return {"output": {"text": response.choices[0].message.content}}
            except Exception as e2:
                logging.error(f"Retry (no temperature) also failed: {repr(e2)}")
                raise Exception(f"DeepSeek API error: {str(e2)}")
        
        raise Exception(f"DeepSeek API error: {str(e)}")


def call_openai_api(prompt: str, model: str = "gpt-4.1", temperature: float = 0.1) -> Dict:
    """è°ƒç”¨OpenAI API
    
    Args:
        prompt: è¾“å…¥prompt
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        æ ‡å‡†åŒ–çš„å“åº”å­—å…¸ {"output": {"text": "..."}}
        
    Raises:
        Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        from openai import OpenAI
    except ImportError:
        raise Exception("è¯·å®‰è£… openai åº“: pip install openai")
    
    client = OpenAI(api_key=API_CONFIG.openai_api_key)
    
    # æŸäº›æ¨¡å‹ä¸æ”¯æŒtemperatureå‚æ•°
    temp_to_send = None if model == "gpt-5" else temperature
    
    try:
        kwargs = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        if temp_to_send is not None:
            kwargs["temperature"] = temp_to_send
            
        response = client.chat.completions.create(**kwargs)
        return {"output": {"text": response.choices[0].message.content}}
    except Exception as e:
        logging.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {repr(e)}")
        
        # å¦‚æœæ˜¯temperatureå‚æ•°é—®é¢˜ï¼Œå°è¯•ä¸å¸¦temperatureé‡è¯•
        err_text = repr(e).lower()
        if "temperature" in err_text or "unsupported" in err_text:
            try:
                kwargs = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                }
                response = client.chat.completions.create(**kwargs)
                return {"output": {"text": response.choices[0].message.content}}
            except Exception as e2:
                logging.error(f"Retry (no temperature) also failed: {repr(e2)}")
                raise Exception(f"OpenAI API error: {str(e2)}")
        
        raise Exception(f"OpenAI API error: {str(e)}")


def call_gemini_api(prompt: str, model: str = "gemini-2.5-pro", temperature: float = 0.1) -> Dict:
    """è°ƒç”¨Gemini API
    
    Args:
        prompt: è¾“å…¥prompt
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        æ ‡å‡†åŒ–çš„å“åº”å­—å…¸ {"output": {"text": "..."}}
        
    Raises:
        Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        from openai import OpenAI
    except ImportError:
        raise Exception("è¯·å®‰è£… openai åº“: pip install openai")
    
    try:
        client = OpenAI(api_key=API_CONFIG.gemini_api_key, base_url=API_CONFIG.gemini_url)
        kwargs = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        if temperature is not None:
            kwargs["temperature"] = temperature
            
        response = client.chat.completions.create(**kwargs)
        return {"output": {"text": response.choices[0].message.content}}
    except Exception as e:
        logging.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {repr(e)}")
        
        # å¦‚æœæ˜¯temperatureå‚æ•°é—®é¢˜ï¼Œå°è¯•ä¸å¸¦temperatureé‡è¯•
        err_text = repr(e).lower()
        if "temperature" in err_text or "unsupported" in err_text:
            try:
                kwargs = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                }
                response = client.chat.completions.create(**kwargs)
                return {"output": {"text": response.choices[0].message.content}}
            except Exception as e2:
                logging.error(f"Retry (no temperature) also failed: {repr(e2)}")
                raise Exception(f"Gemini API error: {str(e2)}")
        
        raise Exception(f"Gemini API error: {str(e)}")

def call_llm_api(
    prompt: str,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> Dict:
    """è°ƒç”¨LLM APIçš„ç»Ÿä¸€æ¥å£
    
    Args:
        prompt: è¾“å…¥promptæ–‡æœ¬
        model: æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–ModelTypeæšä¸¾å€¼
        config: è¯Šæ–­é…ç½®å¯¹è±¡
        
    Returns:
        è§£æåçš„JSONå“åº”
        
    Raises:
        Exception: APIè°ƒç”¨æˆ–è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    if config is None:
        config = DiagnosisConfig()
    
    # æ¸…ç†promptä¸­çš„ç‰¹æ®Šå­—ç¬¦
    prompt = clean_prompt(prompt)
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(config.max_retries):
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨å¯¹åº”çš„API
            if model == ModelType.QWEN or model == "qwen":
                try:
                    import dashscope
                    from dashscope import Generation
                    # è®¾ç½®APIå¯†é’¥ï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
                    if API_CONFIG.dashscope_api_key and not dashscope.api_key:
                        dashscope.api_key = API_CONFIG.dashscope_api_key
                except ImportError:
                    raise Exception("è¯·å®‰è£… dashscope åº“: pip install dashscope")
                
                response = Generation.call(
                    model="qwen-max",
                    prompt=prompt,
                    temperature=config.temperature,
                    result_format="json",
                    timeout=config.timeout
                )
            elif model in [ModelType.GPT_4_1, ModelType.GPT_5, "gpt-4.1", "gpt-5"]:
                response = call_openai_api(
                    prompt,
                    model=model,
                    temperature=config.temperature
                )
            elif model == ModelType.DEEPSEEK or model == "deepseek":
                response = call_deepseek_api(
                    prompt,
                    temperature=config.temperature
                )
            else:
                response = call_gemini_api(
                    prompt,
                    temperature=config.temperature
                )
            break
        except (RequestException, Timeout, KeyboardInterrupt) as e:
            if attempt < config.max_retries - 1:
                logging.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{config.max_retries}: {str(e)}")
                time.sleep(config.retry_delay)
                continue
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•{config.max_retries}æ¬¡ï¼‰: {str(e)}")
    
    # è§£æå“åº”
    try:
        if model == ModelType.QWEN or model == "qwen":
            response_text = response.output.text.strip()
        else:
            response_text = response["output"]["text"].strip()
        
        return extract_json_from_response(response_text)
    except Exception as e:
        raise Exception(f"è§£æå“åº”å¤±è´¥: {str(e)}, åŸå§‹å“åº”: {response_text[:200]}")


# ============================================================================
# è¯Šæ–­é˜¶æ®µå‡½æ•°
# ============================================================================

def _print_stage_header(stage_name: str, stage_number: int = 0):
    """æ‰“å°é˜¶æ®µæ ‡é¢˜"""
    print("=" * 60)
    print(f"é˜¶æ®µ{stage_number}: {stage_name}")
    print("=" * 60)


def stage0_consistency_check(
    qa_data: QAData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> StageResult:
    """é˜¶æ®µ0ï¼šä¸€è‡´æ€§æ£€æŸ¥
    
    æ£€æŸ¥æ¨¡å‹å›ç­”æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageResultå¯¹è±¡ï¼ŒåŒ…å«è¯Šæ–­ç»“æœ
    """
    _print_stage_header("ä¸€è‡´æ€§æ£€æŸ¥", 0)
    
    qa_question_str = qa_data.to_json_str("question")
    qa_answer_str = qa_data.to_json_str("answer")
    qa_response_str = qa_data.to_json_str("response")
    
    prompt = f"""
You are an evaluation assistant. Determine whether qa_response is semantically consistent with qa_answer.

Consistency rules:
- All key information in qa_answer must appear in qa_response.
- Missing, incorrect or unclear details make it inconsistent.

Example :
qa_answer: "first weekend of August 2023"
qa_response: "5 August 2023."
â†’ inconsistent (incorrectly narrows the time range)

Now evaluate:
input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}

Output:
{{
  "is_consistent": true/false,
  "reason": "brief explanation"
}}
"""
    
    try:
        result = call_llm_api(prompt, model, config)
        is_consistent = result.get("is_consistent", False)
        
        stage_result = StageResult(
            stage_passed=is_consistent,
            label=None if is_consistent else "inconsistent",
            reason=result.get("reason", ""),
            stage=DiagnosisStage.CONSISTENCY_CHECK
        )
        
        print(f"âœ“ ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ: {'é€šè¿‡' if is_consistent else 'ä¸é€šè¿‡'}")
        print(f"  åŸå› : {stage_result.reason}\n")
        
        return stage_result
    except Exception as e:
        logging.error(f"é˜¶æ®µ0é”™è¯¯: {str(e)}")
        return StageResult(
            stage_passed=False,
            label=None,
            reason=f"é˜¶æ®µ0é”™è¯¯: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


def stage1_memory_extraction(
    qa_data: QAData,
    memory_data: MemoryData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> StageResult:
    """é˜¶æ®µ1ï¼šè®°å¿†æå–é˜¶æ®µ
    
    æ£€æŸ¥åˆå§‹è®°å¿†æå–æ˜¯å¦å……åˆ†
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        memory_data: è®°å¿†æ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageResultå¯¹è±¡ï¼ŒåŒ…å«è¯Šæ–­ç»“æœ
    """
    _print_stage_header("è®°å¿†æå–é˜¶æ®µ", 1)
    
    qa_question_str = qa_data.to_json_str("question")
    qa_answer_str = qa_data.to_json_str("answer")
    qa_response_str = qa_data.to_json_str("response")
    # é˜¶æ®µ1åªçœ‹åˆå§‹æå–ç»“æœï¼ŒåŒæ—¶ä¿ç•™time_stampå­—æ®µï¼Œæ–¹ä¾¿è¿›è¡Œæ—¶é—´ç›¸å…³åˆ¤æ–­
    memories1_initial_results = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "initial_results": item.get("initial_results", []),
        }
        for item in memory_data.person1_memories
    ]
    memories2_initial_results = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "initial_results": item.get("initial_results", []),
        }
        for item in memory_data.person2_memories
    ]
    memories1_str = json.dumps(memories1_initial_results, ensure_ascii=False)
    memories2_str = json.dumps(memories2_initial_results, ensure_ascii=False)
    
    prompt = f"""
You are an evaluation assistant for the Memory Extraction Stage.
Task:
1. Use their initial_results (and time_stamp if needed) to determine whether the extracted memories are sufficient to answer qa_question.
2. If sufficient â†’ is_sufficient = true (label = null).
3. If insufficient, classify the issue:
   - "1.1": Missing key information
   - "1.2": Incorrect or conflicting information
   - "1.3": Ambiguous or overly generic information

Examples:

Example 1:
qa_question: "Where did Caroline move from 4 years ago?"
qa_answer: "Sweden"
qa_response: "home country"
person1_memories: {{"initial_results": ["Caroline moved from her home country 4 years ago"]}}
person2_memories: {{"initial_results": []}}
Output:
{{
  "is_sufficient": false,
  "label": "1.1",
  "reason": "The extracted memory only says 'home country' and lacks the specific detail 'Sweden.'"
}}

Example 2:
qa_question: "What kind of films does Joanna enjoy?"
qa_answer: "Dramas and emotionally-driven films"
qa_response: "dramas and romantic comedies"
person1_memories: {{"initial_results": ["Joanna enjoys dramas and emotionally-driven films."]}}
person2_memories: {{"initial_results": ["Joanna enjoys dramas and romantic comedies."]}}
Output:
{{
  "is_sufficient": false,
  "label": "1.2",
  "reason": "The memories conflictâ€”one mentions emotionally-driven films, the other romantic comediesâ€”indicating incorrect/inconsistent extraction."
}}

Example 3:
qa_question: "What food item did Maria drop off at the homeless shelter?"
qa_answer: "Cakes"
qa_response: "baked goods"
person1_memories: {{"initial_results": ["Maria dropped off baked goods..."]}}
person2_memories: {{"initial_results": ["Maria dropped off baked goods..."]}}
Output:
{{
  "is_sufficient": false,
  "label": "1.3",
  "reason": "The extracted memory is too vague ('baked goods') and does not specify 'cakes.'"
}}

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- person1_memories: {memories1_str}
- person2_memories: {memories2_str}

Output format:
{{
  "is_sufficient": true/false,
  "label": "1.1" or "1.2" or "1.3" or null,
  "reason": "Detailed explanation"
}}

"""
    
    try:
        result = call_llm_api(prompt, model, config)
        is_sufficient = result.get("is_sufficient", False)
        
        stage_result = StageResult(
            stage_passed=is_sufficient,
            label=None if is_sufficient else result.get("label"),
            reason=result.get("reason", ""),
            stage=DiagnosisStage.MEMORY_EXTRACTION
        )
        
        print(f"âœ“ è®°å¿†æå–ç»“æœ: {'é€šè¿‡' if is_sufficient else 'ä¸é€šè¿‡'}")
        if not is_sufficient:
            print(f"  é—®é¢˜ç±»å‹: {stage_result.label}")
        print(f"  åŸå› : {stage_result.reason}\n")
        
        return stage_result
    except Exception as e:
        logging.error(f"é˜¶æ®µ1é”™è¯¯: {str(e)}")
        return StageResult(
            stage_passed=False,
            label=None,
            reason=f"é˜¶æ®µ1é”™è¯¯: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


def stage2_memory_update(
    qa_data: QAData,
    memory_data: MemoryData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> StageResult:
    """é˜¶æ®µ2ï¼šè®°å¿†æ›´æ–°é˜¶æ®µ
    
    æ£€æŸ¥è®°å¿†æ›´æ–°è¿‡ç¨‹æ˜¯å¦æ­£ç¡®
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        memory_data: è®°å¿†æ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageResultå¯¹è±¡ï¼ŒåŒ…å«è¯Šæ–­ç»“æœ
    """
    _print_stage_header("è®°å¿†æ›´æ–°é˜¶æ®µ", 2)
    
    qa_question_str = qa_data.to_json_str("question")
    qa_answer_str = qa_data.to_json_str("answer")
    qa_response_str = qa_data.to_json_str("response")
    # é˜¶æ®µ2åªçœ‹æ›´æ–°é“¾ï¼ŒåŒæ—¶ä¿ç•™time_stampå­—æ®µï¼Œæ–¹ä¾¿ç»“åˆæ—¶é—´åˆ¤æ–­æ›´æ–°æ˜¯å¦åˆç†
    memories1_update_chains = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "update_chain": item.get("update_chain", []),
        }
        for item in memory_data.person1_memories
    ]
    memories2_update_chains = [
        {
            "time_stamp": item.get("time_stamp", ""),
            "update_chain": item.get("update_chain", []),
        }
        for item in memory_data.person2_memories
    ]
    memories1_str = json.dumps(memories1_update_chains, ensure_ascii=False)
    memories2_str = json.dumps(memories2_update_chains, ensure_ascii=False)
    
    prompt = f"""
You are an evaluation assistant for the Memory Update Stage.
Task:
1. From the update_chain, use only the final updated memory for each item.
2. Determine whether the updated memories contain sufficient and correct information to answer qa_question.
3. If sufficient â†’ is_sufficient = true (label = null).
4. If insufficient, classify the issue according to the update error type:
   - "2.1": Incorrect update (added wrong or fabricated details)
   - "2.2": Deleted information (removed necessary memory entries)
   - "2.3": Weakened information (kept but diluted or less specific)

Examples:

Example 1:
qa_question: "What did James prepare for the first time in the cooking class?"
qa_answer: "Omelette"
qa_response: "omelette, meringue, dough"
update_chain: [{{
  "event": "UPDATE",
  "memory": "James ... made an omelette ... He also made meringue and learned how to make dough.",
  "previous_memory": "James ... made a great omelette for the first time."
}}]
Output:
{{
  "is_sufficient": false,
  "label": "2.1",
  "reason": "The update introduces incorrect new first-time dishesâ€”meringue and doughâ€”contradicting the original memory."
}}

Example 2:
qa_question: "When did Maria adopt Shadow?"
qa_answer: "The week before 13 August 2023"
qa_response: "13 August, 2023"
update_chain: [{{
  "event": "DELETE",
  "memory": "Maria adopted a cute puppy from a shelter last week, and she feels blessed to give her a home."
}}]
Output:
{{
  "is_sufficient": false,
  "label": "2.2",
  "reason": "Because the update event is a DELETE operation, it removes the memory stating that Maria adopted the puppy the previous week, eliminating the key information needed to infer the correct adoption timeframe."
}}

Example 3:
qa_question: "How many times has Jolene been to France?"
qa_answer: "two times"
qa_response: "None."
update_chain: [{{
  "event": "UPDATE",
  "memory": "Jolene has a pendant that represents freedom...",
  "previous_memory": "Jolene has a pendant her mother gave her in 2010 in Paris."
}}]
Output:
{{
  "is_sufficient": false,
  "label": "2.3",
  "reason": "The update removes the Paris detail, weakening the information needed to infer her past visits to France."
}}

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- person1_memories: {memories1_str}
- person2_memories: {memories2_str}

Output format:
{{
  "is_sufficient": true/false,
  "label": "2.1" or "2.2" or "2.3" or null,
  "reason": "Detailed explanation"
}}
"""
    
    try:
        result = call_llm_api(prompt, model, config)
        is_sufficient = result.get("is_sufficient", False)
        
        stage_result = StageResult(
            stage_passed=is_sufficient,
            label=None if is_sufficient else result.get("label"),
            reason=result.get("reason", ""),
            stage=DiagnosisStage.MEMORY_UPDATE
        )
        
        print(f"âœ“ è®°å¿†æ›´æ–°ç»“æœ: {'é€šè¿‡' if is_sufficient else 'ä¸é€šè¿‡'}")
        if not is_sufficient:
            print(f"  é—®é¢˜ç±»å‹: {stage_result.label}")
        print(f"  åŸå› : {stage_result.reason}\n")
        
        return stage_result
    except Exception as e:
        logging.error(f"é˜¶æ®µ2é”™è¯¯: {str(e)}")
        return StageResult(
            stage_passed=False,
            label=None,
            reason=f"é˜¶æ®µ2é”™è¯¯: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


def stage3_memory_retrieval(
    qa_data: QAData,
    memory_data: MemoryData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> StageResult:
    """é˜¶æ®µ3ï¼šè®°å¿†æ£€ç´¢é˜¶æ®µ
    
    æ£€æŸ¥è®°å¿†æ£€ç´¢æ˜¯å¦æ­£ç¡®
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        memory_data: è®°å¿†æ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageResultå¯¹è±¡ï¼ŒåŒ…å«è¯Šæ–­ç»“æœ
    """
    _print_stage_header("è®°å¿†æ£€ç´¢é˜¶æ®µ", 3)
    
    qa_question_str = qa_data.to_json_str("question")
    qa_answer_str = qa_data.to_json_str("answer")
    speaker1_memories_str = memory_data.to_json_str("speaker1_retrieval")
    speaker2_memories_str = memory_data.to_json_str("speaker2_retrieval")
    
    prompt = f"""
You are an evaluation assistant for the Memory Retrieval Stage.
Task:
Based strictly on speaker1_retrieval and speaker2_retrieval:
1. Determine whether the retrieved memories contain enough correct information to answer qa_question.
2. If sufficient â†’ is_sufficient = true (label = null).
3. If insufficient, determine the retrieval issue:
   - "3.1": Failed to recall correct information (missing the key facts)
   - "3.2": Unreasonable ranking (recalled irrelevant/common info while missing the most relevant facts)

Examples:

Example 1:
qa_question: "How does Melanie prioritize self-care?"
qa_answer: "by carving out some me-time each day for activities like running, reading, or playing the violin"
qa_response: "Running, pottery, charity races."
speaker1_retrieval: [
  "Melanie prioritizes her mental health...",
  "Melanie enjoys running as a way to de-stress...",
  "Melanie is thankful for her family..."
]
speaker2_retrieval: [
  "Melanie finds self-care to be a work in progress...",
  "Melanie has been running longer...",
  "Melanie values mental health..."
]
Output:
{{
  "is_sufficient": false,
  "label": "3.1",
  "reason": "The retrieved memories mention running and mental-health efforts but miss key self-care details such as reading, violin, and daily me-time."
}}

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- speaker1_retrieval: {speaker1_memories_str}
- speaker2_retrieval: {speaker2_memories_str}

Output format:
{{
  "is_sufficient": true/false,
  "label": "3.1" or "3.2" or null,
  "reason": "Detailed explanation"
}}
"""
    
    try:
        result = call_llm_api(prompt, model, config)
        is_sufficient = result.get("is_sufficient", False)
        
        stage_result = StageResult(
            stage_passed=is_sufficient,
            label=None if is_sufficient else result.get("label"),
            reason=result.get("reason", ""),
            stage=DiagnosisStage.MEMORY_RETRIEVAL
        )
        
        print(f"âœ“ è®°å¿†æ£€ç´¢ç»“æœ: {'é€šè¿‡' if is_sufficient else 'ä¸é€šè¿‡'}")
        if not is_sufficient:
            print(f"  é—®é¢˜ç±»å‹: {stage_result.label}")
        print(f"  åŸå› : {stage_result.reason}\n")
        
        return stage_result
    except Exception as e:
        logging.error(f"é˜¶æ®µ3é”™è¯¯: {str(e)}")
        return StageResult(
            stage_passed=False,
            label=None,
            reason=f"é˜¶æ®µ3é”™è¯¯: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


def stage4_reasoning(
    qa_data: QAData,
    memory_data: MemoryData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> StageResult:
    """é˜¶æ®µ4ï¼šæ¨ç†é˜¶æ®µ
    
    å¦‚æœå‰é¢é˜¶æ®µéƒ½é€šè¿‡ï¼Œé—®é¢˜å‡ºåœ¨æ¨ç†ç¯èŠ‚
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        memory_data: è®°å¿†æ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        StageResultå¯¹è±¡ï¼ŒåŒ…å«è¯Šæ–­ç»“æœ
    """
    _print_stage_header("æ¨ç†é˜¶æ®µ", 4)
    
    qa_question_str = qa_data.to_json_str("question")
    qa_answer_str = qa_data.to_json_str("answer")
    qa_response_str = qa_data.to_json_str("response")
    speaker1_memories_str = memory_data.to_json_str("speaker1_retrieval")
    speaker2_memories_str = memory_data.to_json_str("speaker2_retrieval")
    
    prompt = f"""
You are an evaluation assistant for the Reasoning Stage.

Context:
All previous stages (extraction, update, retrieval) have passed, meaning the model had sufficient correct information.  
If qa_response still does not match qa_answer, the issue is a reasoning error.

Task:
Based on qa_question, qa_answer, qa_response, and the retrieved memories, classify the reasoning issue:
- "4.1": Correct memory entries were ignored (model overlooks correct memory entries present in retrieval)
- "4.2": Reasoning error (model invents details, over-specifies, or makes unsupported inferences)
- "4.3": Format or detail error (minor deviations such as missing qualifiers or altered phrasing that slightly change meaning)

Examples:

Example 1:
qa_question: "What does Melanie do with her family on hikes?"
qa_answer: "Roast marshmallows, tell stories"
qa_response: "explore nature and bond"
speaker1_retrieval: ["Melanie prioritizes her mental health..."]
speaker2_retrieval: ["Melanie ... roasted marshmallows ... and told stories..."]
Output:
{{
  "label": "4.1",
  "reason": "The retrieved memory clearly includes roasting marshmallows and telling stories, but the model ignored this memory entry"
}}

Example 2:
qa_question: "When did Caroline have a picnic?"
qa_answer: "The week before 6 July 2023"
qa_response: "29 June 2023."
Output:
{{
  "label": "4.2",
  "reason": "The answer only specifies a time range, but the model unjustifiably inferred an exact date."
}}

Example 3:
qa_question: "How often does John see sunsets like the one he shared with Maria?"
qa_answer: "At least once a week"
qa_response: "once a week"
Output:
{{
  "label": "4.3",
  "reason": "The model dropped the qualifier 'at least,' slightly altering the meaning."
}}

Now evaluate the following:

Input:
- qa_question: {qa_question_str}
- qa_answer: {qa_answer_str}
- qa_response: {qa_response_str}
- speaker1_retrieval: {speaker1_memories_str}
- speaker2_retrieval: {speaker2_memories_str}

Output format:
{{
  "label": "4.1" or "4.2" or "4.3",
  "reason": "Detailed explanation"
}}
"""
    
    try:
        result = call_llm_api(prompt, model, config)
        
        stage_result = StageResult(
            stage_passed=False,
            label=result.get("label"),
            reason=result.get("reason", ""),
            stage=DiagnosisStage.REASONING
        )
        
        print(f"âœ“ æ¨ç†é—®é¢˜ç±»å‹: {stage_result.label}")
        print(f"  åŸå› : {stage_result.reason}\n")
        
        return stage_result
    except Exception as e:
        logging.error(f"é˜¶æ®µ4é”™è¯¯: {str(e)}")
        return StageResult(
            stage_passed=False,
            label="4.2",
            reason=f"é˜¶æ®µ4é”™è¯¯: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


# ============================================================================
# ä¸»è¯Šæ–­å‡½æ•°
# ============================================================================

def analyze_qa_pair(
    qa_data: QAData,
    memory_data: MemoryData,
    model: str = "deepseek",
    config: Optional[DiagnosisConfig] = None
) -> DiagnosisResult:
    """åˆ†é˜¶æ®µè¯Šæ–­ç³»ç»Ÿä¸»å‡½æ•°
    
    æŒ‰é¡ºåºæ‰§è¡Œå„ä¸ªè¯Šæ–­é˜¶æ®µï¼Œç›´åˆ°å‘ç°é—®é¢˜æˆ–å…¨éƒ¨é€šè¿‡
    
    Args:
        qa_data: QAæ•°æ®å¯¹è±¡
        memory_data: è®°å¿†æ•°æ®å¯¹è±¡
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: è¯Šæ–­é…ç½®
        
    Returns:
        DiagnosisResultå¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„è¯Šæ–­ç»“æœ
    """
    print(f"\n{'='*70}")
    print(f"ğŸ” å¼€å§‹åˆ†é˜¶æ®µè¯Šæ–­")
    print(f"ğŸ“ é—®é¢˜: {qa_data.question}")
    print(f"{'='*70}\n")
    
    try:
        # é˜¶æ®µ0: ä¸€è‡´æ€§æ£€æŸ¥
        stage0_result = stage0_consistency_check(qa_data, model, config)
        if stage0_result.stage_passed:
            return DiagnosisResult(
                label=None,
                reason=stage0_result.reason,
                stage=DiagnosisStage.CONSISTENCY_CHECK
            )
        
        # é˜¶æ®µ1: è®°å¿†æå–é˜¶æ®µ
        stage1_result = stage1_memory_extraction(qa_data, memory_data, model, config)
        if not stage1_result.stage_passed:
            return DiagnosisResult(
                label=stage1_result.label,
                reason=stage1_result.reason,
                stage=DiagnosisStage.MEMORY_EXTRACTION
            )
        
        # é˜¶æ®µ2: è®°å¿†æ›´æ–°é˜¶æ®µ
        stage2_result = stage2_memory_update(qa_data, memory_data, model, config)
        if not stage2_result.stage_passed:
            return DiagnosisResult(
                label=stage2_result.label,
                reason=stage2_result.reason,
                stage=DiagnosisStage.MEMORY_UPDATE
            )
        
        # é˜¶æ®µ3: è®°å¿†æ£€ç´¢é˜¶æ®µ
        stage3_result = stage3_memory_retrieval(qa_data, memory_data, model, config)
        if not stage3_result.stage_passed:
            return DiagnosisResult(
                label=stage3_result.label,
                reason=stage3_result.reason,
                stage=DiagnosisStage.MEMORY_RETRIEVAL
            )
        
        # é˜¶æ®µ4: æ¨ç†é˜¶æ®µï¼ˆå‰é¢éƒ½é€šè¿‡äº†ï¼Œé—®é¢˜åœ¨æ¨ç†ï¼‰
        stage4_result = stage4_reasoning(qa_data, memory_data, model, config)
        return DiagnosisResult(
            label=stage4_result.label,
            reason=stage4_result.reason,
            stage=DiagnosisStage.REASONING
        )
        
    except Exception as e:
        logging.error(f"è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return DiagnosisResult(
            label=None,
            reason=f"è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {str(e)}",
            stage=DiagnosisStage.ERROR
        )


def analyze_qa_pair_legacy(
    qa_question: str,
    qa_answer: str,
    qa_response: str,
    memories1: List[dict],
    memories2: List[dict],
    speaker1_memories: List[Dict],
    speaker2_memories: List[Dict],
    model: str = "deepseek"
) -> Dict:
    """å…¼å®¹æ—§æ¥å£çš„åˆ†æå‡½æ•°
    
    è¯¥å‡½æ•°ä¿æŒä¸åŸæœ‰ä»£ç çš„å…¼å®¹æ€§ï¼Œå°†å‚æ•°è½¬æ¢ä¸ºæ–°çš„æ•°æ®ç±»åè°ƒç”¨æ–°çš„åˆ†æå‡½æ•°
    
    Args:
        qa_question: é—®é¢˜æ–‡æœ¬
        qa_answer: å‚è€ƒç­”æ¡ˆ
        qa_response: æ¨¡å‹å›ç­”
        memories1: person1çš„è®°å¿†æ•°æ®
        memories2: person2çš„è®°å¿†æ•°æ®
        speaker1_memories: speaker1çš„æ£€ç´¢è®°å¿†
        speaker2_memories: speaker2çš„æ£€ç´¢è®°å¿†
        model: ä½¿ç”¨çš„æ¨¡å‹
        
    Returns:
        è¯Šæ–­ç»“æœå­—å…¸ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
    """
    # åˆ›å»ºæ•°æ®å¯¹è±¡
    qa_data = QAData(
        question=qa_question,
        answer=qa_answer,
        response=qa_response
    )
    
    memory_data = MemoryData(
        person1_memories=memories1,
        person2_memories=memories2,
        speaker1_retrieval=speaker1_memories,
        speaker2_retrieval=speaker2_memories
    )
    
    # è°ƒç”¨æ–°å‡½æ•°
    result = analyze_qa_pair(qa_data, memory_data, model)
    
    # è½¬æ¢ä¸ºæ—§æ ¼å¼
    return {
        "label": result.label,
        "reason": result.reason,
        "stage": result.stage.value if isinstance(result.stage, DiagnosisStage) else result.stage
    }

def analyze_qa_pair_with_voting(
    qa_question: str,
    qa_answer: str,
    qa_response: str,
    memories1: List[dict],
    memories2: List[dict],
    speaker1_memories: List[Dict],
    speaker2_memories: List[Dict],
    model: str = "deepseek",
    num_votes: int = 3
) -> Dict:
    """ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶åˆ†æQAå¯¹å’Œæ£€ç´¢è®°å¿†
    
    Args:
        qa_question: é—®é¢˜æ–‡æœ¬
        qa_answer: å‚è€ƒç­”æ¡ˆ
        qa_response: æ¨¡å‹å›ç­”
        memories1: person1çš„è®°å¿†æ•°æ®
        memories2: person2çš„è®°å¿†æ•°æ®
        speaker1_memories: speaker1çš„æ£€ç´¢è®°å¿†
        speaker2_memories: speaker2çš„æ£€ç´¢è®°å¿†
        model: ä¸»è¦ä½¿ç”¨çš„æ¨¡å‹
        num_votes: æŠ•ç¥¨è½®æ•°
        
    Returns:
        åŒ…å«æœ€ç»ˆè¯Šæ–­ç»“æœå’ŒæŠ•ç¥¨è¯¦æƒ…çš„å­—å…¸
    """
    print(f"\nğŸ—³ï¸  é—®é¢˜: {qa_question}")
    print(f"ğŸ“Š ä½¿ç”¨ {model} ä½œä¸ºä¸»æ¨¡å‹è¿›è¡Œ {num_votes} è½®æŠ•ç¥¨ï¼ˆæ¯è½®ä½¿ç”¨ä¸åŒæ¨¡å‹ï¼‰\n")
    
    # åˆ›å»ºæ•°æ®å¯¹è±¡ï¼ˆåªéœ€åˆ›å»ºä¸€æ¬¡ï¼Œå¯å¤ç”¨ï¼‰
    qa_data = QAData(
        question=qa_question,
        answer=qa_answer,
        response=qa_response
    )
    
    memory_data = MemoryData(
        person1_memories=memories1,
        person2_memories=memories2,
        speaker1_retrieval=speaker1_memories,
        speaker2_retrieval=speaker2_memories
    )
    
    # å­˜å‚¨æ¯è½®çš„ç»“æœ
    vote_results = []
    
    # å®šä¹‰æ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºè½®æ¢
    models = ["deepseek", "gpt-4.1", "gpt-5"]
    
    # ç¡®ä¿ä¸»æ¨¡å‹åœ¨åˆ—è¡¨ä¸­ï¼Œå¦‚æœä¸åœ¨åˆ™æ·»åŠ 
    if model not in models:
        models.insert(0, model)
    else:
        # å°†ä¸»æ¨¡å‹ç§»åˆ°é¦–ä½
        models.remove(model)
        models.insert(0, model)
    
    # è¿›è¡Œå¤šè½®æŠ•ç¥¨ï¼Œç¡®ä¿æ¯è½®ä½¿ç”¨ä¸åŒæ¨¡å‹
    used_models = []
    for i in range(num_votes):
        # é€‰æ‹©æ¨¡å‹ï¼šä¼˜å…ˆä½¿ç”¨æœªä½¿ç”¨è¿‡çš„æ¨¡å‹
        current_model = None
        for m in models:
            if m not in used_models:
                current_model = m
                break
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å·²ä½¿ç”¨è¿‡ï¼Œåˆ™ä»é™¤äº†ä¸»æ¨¡å‹å¤–çš„æ¨¡å‹ä¸­é€‰æ‹©
        if current_model is None:
            unused_models = [m for m in models if m != model]
            if unused_models:
                current_model = unused_models[len(used_models) % len(unused_models)]
            else:
                current_model = models[len(used_models) % len(models)]
        
        used_models.append(current_model)
        print(f"ğŸ”„ ç¬¬ {i+1}/{num_votes} è½®åˆ†æï¼Œä½¿ç”¨æ¨¡å‹: {current_model}")
        
        try:
            # ä½¿ç”¨æ–°çš„æ•°æ®ç±»æ¥å£
            result = analyze_qa_pair(qa_data, memory_data, model=current_model)
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼å¹¶æ·»åŠ ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
            result_dict = {
                "label": result.label,
                "reason": result.reason,
                "stage": result.stage.value if isinstance(result.stage, DiagnosisStage) else result.stage,
                "used_model": current_model
            }
            vote_results.append(result_dict)
            print(f"   âœ… ç¬¬ {i+1} è½®å®Œæˆ: label={result.label}, model={current_model}\n")
        except Exception as e:
            logging.error(f"ç¬¬ {i+1} è½®åˆ†æå¤±è´¥: {str(e)}")
            print(f"   âŒ ç¬¬ {i+1} è½®åˆ†æå¤±è´¥: {str(e)}\n")
            # å¦‚æœæŸä¸€è½®å¤±è´¥ï¼Œæ·»åŠ ä¸€ä¸ªé»˜è®¤ç»“æœï¼ˆæ ‡ç­¾ä¸ºnullï¼‰
            vote_results.append({
                "label": None,
                "reason": f"APIè°ƒç”¨å¤±è´¥: {str(e)}",
                "stage": "error",
                "used_model": current_model
            })
    
    # ç»Ÿè®¡æŠ•ç¥¨ç»“æœï¼ˆåŒ…å«Noneæ ‡ç­¾ï¼‰
    labels = [result["label"] for result in vote_results]
    
    # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾ï¼ˆåŒ…æ‹¬Noneï¼‰
    label_counter = Counter(labels)
    most_common_label = label_counter.most_common(1)[0][0]

    
    # è·å–æœ€ç»ˆç»“æœçš„è¯¦ç»†ä¿¡æ¯
    final_result = None
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŠ•ç¥¨ç»“æœéƒ½ä¸åŒï¼ˆå³æ²¡æœ‰é‡å¤çš„æ ‡ç­¾ï¼‰
    all_different = len(label_counter) == len(vote_results) and len(vote_results) > 1
    
    if all_different:
        # å¦‚æœæ‰€æœ‰æŠ•ç¥¨ç»“æœéƒ½ä¸åŒï¼Œåˆ™ä½¿ç”¨ä¸»æ¨¡å‹çš„ç»“æœ
        print(f"æ‰€æœ‰æŠ•ç¥¨ç»“æœéƒ½ä¸åŒï¼Œä½¿ç”¨ä¸»æ¨¡å‹ {model} çš„ç»“æœ")
        for result in vote_results:
            if result.get("used_model") == model:
                final_result = result
                most_common_label = result["label"]
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»æ¨¡å‹çš„ç»“æœï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ
        if final_result is None:
            print(f"æœªæ‰¾åˆ°ä¸»æ¨¡å‹ {model} ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ")
            final_result = vote_results[0]
            most_common_label = final_result["label"]
    else:
        # æ ¹æ®å¾—ç¥¨æ•°æœ€å¤šçš„æ ‡ç­¾æ¥é€‰æ‹©æœ€ç»ˆç»“æœ
        for result in vote_results:
            if result["label"] == most_common_label:
                final_result = result
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ
        if final_result is None and vote_results:
            final_result = vote_results[0]
    
    # ç®€åŒ– voting_detailsï¼ŒåªåŒ…å«æ¯è½®çš„æ ‡æ³¨ç»“æœ
    final_result["voting_details"] = {
        "label_votes": dict(label_counter),
        "individual_results": [
            {
                "label": result["label"],
                "used_model": result.get("used_model", "unknown"),
                "reason": result.get("reason", "")
            } 
            for result in vote_results
        ],
        "all_different": all_different  # æ·»åŠ æ ‡è®°ï¼Œè¡¨ç¤ºæ˜¯å¦æ‰€æœ‰ç»“æœéƒ½ä¸åŒ
    }
    
    # æ‰“å°æŠ•ç¥¨æ±‡æ€»
    print(f"{'='*70}")
    print(f"ğŸ“Š æŠ•ç¥¨æ±‡æ€»")
    print(f"{'='*70}")
    print(f"ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹é¡ºåº: {used_models}")
    
    # å®‰å…¨åœ°æ‰“å°æŠ•ç¥¨ç»“æœ
    vote_count = label_counter[most_common_label] if not all_different else 1
    print(f"ğŸ† æœ€ç»ˆæ ‡ç­¾: {most_common_label} (å¾—ç¥¨æ•°: {vote_count}/{num_votes})")
    if all_different:
        print(f"âš ï¸  æ‰€æœ‰æŠ•ç¥¨ç»“æœéƒ½ä¸åŒï¼Œå·²é€‰æ‹©deepseekæ¨¡å‹çš„ç»“æœ")
    print(f"{'='*70}\n")
    
    return final_result
# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»ç¨‹åºå…¥å£å‡½æ•°
    
    æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼š
        python dignosis.py [model] [options]
        
    å‚æ•°è¯´æ˜ï¼š
        model: å¯é€‰æ¨¡å‹ (deepseek, gpt4.1, gpt5)ï¼Œé»˜è®¤ï¼šdeepseek
        --voting: å¯ç”¨æŠ•ç¥¨æœºåˆ¶ï¼ˆé»˜è®¤ï¼‰
        --no-voting: ç¦ç”¨æŠ•ç¥¨ï¼Œä½¿ç”¨å•ä¸ªæ¨¡å‹
        --num-votes N: æŠ•ç¥¨è½®æ•°ï¼Œé»˜è®¤ï¼š3
        -i, --input: è¾“å…¥æ–‡ä»¶è·¯å¾„
        -o, --output-dir: è¾“å‡ºç›®å½•è·¯å¾„
        -f, --output-file: è¾“å‡ºæ–‡ä»¶å
        
    ç¤ºä¾‹ï¼š
        python diagnosis.py deepseek --no-voting                    # å•æ¨¡å‹è¯Šæ–­
        python diagnosis.py deepseek --voting                       # æŠ•ç¥¨è¯Šæ–­ï¼ˆ3è½®ï¼‰
        python diagnosis.py deepseek --num-votes 5                  # æŠ•ç¥¨è¯Šæ–­ï¼ˆ5è½®ï¼‰
        python diagnosis.py -i data/input.json -o results/         # è‡ªå®šä¹‰è¾“å…¥è¾“å‡º
        python diagnosis.py --input data.json --output-file out.json # æŒ‡å®šæ–‡ä»¶
    """
    import argparse
    import datetime
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description="è®°å¿†è¯Šæ–­ç³»ç»Ÿ - åˆ†é˜¶æ®µè¯Šæ–­QAå¯¹ä¸­çš„é—®é¢˜",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # æ”¯æŒçš„æ¨¡å‹
    model_map = {
        "deepseek": "deepseek",
        "gpt4.1": "gpt-4.1",
        "gpt5": "gpt-5",
    }
    
    # æ·»åŠ å‚æ•°
    parser.add_argument(
        "model",
        nargs="?",
        default="deepseek",
        choices=list(model_map.keys()),
        help="é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: deepseek)"
    )
    
    parser.add_argument(
        "--voting",
        action="store_true",
        default=True,
        help="å¯ç”¨æŠ•ç¥¨æœºåˆ¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    
    parser.add_argument(
        "--no-voting",
        action="store_true",
        help="ç¦ç”¨æŠ•ç¥¨ï¼Œä½¿ç”¨å•ä¸ªæ¨¡å‹è¯Šæ–­"
    )
    
    parser.add_argument(
        "--num-votes",
        type=int,
        default=3,
        help="æŠ•ç¥¨è½®æ•° (é»˜è®¤: 3)"
    )
    
    parser.add_argument(
        "-i", "--input",
        type=str,
        default="data/input/mem0_mem/gpt4omini/mem0_dataset_part1.json",
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/input/mem0_mem/gpt4omini/mem0_dataset_part1.json)"
    )
    
    parser.add_argument(
        "-o", "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: æ ¹æ®è¯Šæ–­æ¨¡å¼è‡ªåŠ¨é€‰æ‹©)"
    )
    
    parser.add_argument(
        "-f", "--output-file",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: æ ¹æ®è¯Šæ–­æ¨¡å¼è‡ªåŠ¨ç”Ÿæˆ)"
    )
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æŠ•ç¥¨
    use_voting = args.voting and not args.no_voting
    
    # è·å–æ¨¡å‹
    model = model_map[args.model]
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸš€ è®°å¿†è¯Šæ–­ç³»ç»Ÿå¯åŠ¨")
    print("="*70)
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ“Š è¯Šæ–­æ¨¡å¼: {'æŠ•ç¥¨æœºåˆ¶ (' + str(args.num_votes) + 'è½®)' if use_voting else 'å•æ¨¡å‹è¯Šæ–­'}")
    print(f"âš™ï¸  é…ç½®: {DiagnosisConfig()}")
    print("="*70 + "\n")
    
    # è®¾ç½®è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_file = args.input
    
    # ä»è¾“å…¥æ–‡ä»¶åä¸­æå–æ ‡è¯†ï¼ˆç”¨äºè¾“å‡ºæ–‡ä»¶å‘½åï¼‰
    input_basename = os.path.splitext(os.path.basename(input_file))[0]
    # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
    input_identifier = input_basename.replace(" ", "_").replace("(", "").replace(")", "")
    
    # æ ¹æ®è¯Šæ–­æ¨¡å¼é€‰æ‹©è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
    if args.output_dir:
        # ç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºç›®å½•
        output_dir = args.output_dir
    else:
        # è‡ªåŠ¨é€‰æ‹©è¾“å‡ºç›®å½•
        if use_voting:
            output_dir = "data/output/llm_annotation_voting"
        else:
            output_dir = "data/output/llm_annotation_single"
    
    # è·å–å½“å‰æ—¶é—´æˆ³
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    if args.output_file:
        # ç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶å
        output_filename = args.output_file
    else:
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆåŒ…å«è¾“å…¥æ–‡ä»¶æ ‡è¯†å’Œæ—¶é—´æˆ³ï¼‰
        if use_voting:
            output_filename = f"{input_identifier}_voting_{args.num_votes}rounds_{model.replace('-', '_')}_{timestamp}.json"
        else:
            output_filename = f"{input_identifier}_single_{model.replace('-', '_')}_{timestamp}.json"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç»„åˆå®Œæ•´çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = os.path.join(output_dir, output_filename)
    
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}\n")
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print(f"ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨ -i å‚æ•°æŒ‡å®šæ­£ç¡®çš„è¾“å…¥æ–‡ä»¶")
        return
    
    # åŠ è½½è¾“å…¥æ•°æ®
    try:
        data = load_json_file(input_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªä¼šè¯\n")
    except Exception as e:
        logging.error(f"åŠ è½½è¾“å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")
        print(f"âŒ é”™è¯¯: æ— æ³•è§£æè¾“å…¥æ–‡ä»¶: {str(e)}")
        return
    
    # åŠ è½½å·²å¤„ç†çš„ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    results = []
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                results = json.load(f)
                logging.info(f"å·²åŠ è½½ {len(results)} æ¡å†å²ç»“æœ")
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logging.warning(f"åŠ è½½å†å²ç»“æœå¤±è´¥: {str(e)}ï¼Œå°†ä»å¤´å¼€å§‹")
            results = []
    
    processed_items = {item["conv_id_question_id"] for item in results}
    
    try:
        total_convs = len(data)
        print(f"ğŸ“Š å¼€å§‹å¤„ç†ï¼Œå…±æœ‰ {total_convs} ä¸ªä¼šè¯éœ€è¦åˆ†æ\n")
        
        for conv_idx, (conv_id, qa_list) in enumerate(data.items(), 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ å¤„ç†ä¼šè¯ {conv_id} ({conv_idx}/{total_convs})")
            print(f"{'='*70}\n")
            
            for qa_idx, qa_item in enumerate(qa_list, 1):
                item_id = f"{conv_id}_{qa_idx-1}"
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                if item_id in processed_items:
                    print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„é—®é¢˜: {item_id}\n")
                    continue
                
                print(f"ğŸ” å¼€å§‹å¤„ç†é—®é¢˜ {qa_idx}/{len(qa_list)}: {item_id}")
                
                try:
                    # æå–æ•°æ®
                    p1 = qa_item.get("person1", {})
                    p2 = qa_item.get("person2", {})
                    memories1 = p1.get("memories", [])
                    memories2 = p2.get("memories", [])
                    
                    # æ ¹æ®é…ç½®é€‰æ‹©è¯Šæ–­æ–¹å¼
                    if use_voting:
                        # ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶
                        analysis = analyze_qa_pair_with_voting(
                            qa_question=qa_item["qa_question"],
                            qa_answer=qa_item["qa_answer"],
                            qa_response=qa_item["qa_response"],
                            memories1=memories1,
                            memories2=memories2,
                            speaker1_memories=qa_item.get("speaker_1_memories", []),
                            speaker2_memories=qa_item.get("speaker_2_memories", []),
                            model=model,
                            num_votes=args.num_votes
                        )
                        
                        # æ„å»ºç»“æœå¯¹è±¡ï¼ˆæŠ•ç¥¨æ¨¡å¼ï¼‰
                        result = {
                            "conv_id_question_id": item_id,
                            "qa_question": qa_item["qa_question"],
                            "qa_answer": qa_item["qa_answer"],
                            "qa_response": qa_item["qa_response"],
                            "qa_category": qa_item.get("qa_category", ""),
                            "label": analysis["label"],
                            "reason": analysis["reason"],
                            "diagnosis_mode": f"voting_{args.num_votes}rounds"
                        }
                        
                        # æ·»åŠ æŠ•ç¥¨è¯¦æƒ…
                        if "voting_details" in analysis:
                            result["voting_details"] = {
                                "label_votes": analysis["voting_details"]["label_votes"],
                                "individual_results": [
                                    {
                                        "label": ir["label"],
                                        "used_model": ir.get("used_model", "unknown"),
                                        "reason": ir.get("reason", "")
                                    }
                                    for ir in analysis["voting_details"]["individual_results"]
                                ],
                                "all_different": analysis["voting_details"].get("all_different", False)
                            }
                    else:
                        # ä½¿ç”¨å•æ¨¡å‹è¯Šæ–­
                        qa_data = QAData(
                            question=qa_item["qa_question"],
                            answer=qa_item["qa_answer"],
                            response=qa_item["qa_response"]
                        )
                        
                        memory_data = MemoryData(
                            person1_memories=memories1,
                            person2_memories=memories2,
                            speaker1_retrieval=qa_item.get("speaker_1_memories", []),
                            speaker2_retrieval=qa_item.get("speaker_2_memories", [])
                        )
                        
                        diagnosis = analyze_qa_pair(qa_data, memory_data, model=model)
                        
                        # æ„å»ºç»“æœå¯¹è±¡ï¼ˆå•æ¨¡å‹æ¨¡å¼ï¼‰
                        result = {
                            "conv_id_question_id": item_id,
                            "qa_question": qa_item["qa_question"],
                            "qa_answer": qa_item["qa_answer"],
                            "qa_response": qa_item["qa_response"],
                            "qa_category": qa_item.get("qa_category", ""),
                            "label": diagnosis.label,
                            "reason": diagnosis.reason,
                            "stage": diagnosis.stage.value if isinstance(diagnosis.stage, DiagnosisStage) else diagnosis.stage,
                            "diagnosis_mode": f"single_model_{model}"
                        }
                    
                    results.append(result)
                    
                    # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    
                    print(f"âœ… é—®é¢˜ {item_id} å¤„ç†å®Œæˆå¹¶å·²ä¿å­˜\n")
                    
                except Exception as e:
                    logging.error(f"å¤„ç†é—®é¢˜ {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    print(f"âŒ å¤„ç†é—®é¢˜ {item_id} å¤±è´¥: {str(e)}\n")
                    continue
                    
    except KeyboardInterrupt:
        print("\nâš ï¸  å¤„ç†å·²ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜...\n")
    except Exception as e:
        logging.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\n")
    finally:
        # ç¡®ä¿æœ€åç»“æœè¢«ä¿å­˜
        if results:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print("\n" + "="*70)
            print("ğŸ‰ å¤„ç†å®Œæˆ")
            print("="*70)
            print(f"âœ… å…±å¤„ç† {len(results)} ä¸ªé—®é¢˜")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            print("="*70 + "\n")


if __name__ == "__main__":
    main()